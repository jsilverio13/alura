# -*- coding: utf-8 -*-
"""deep-learning-into-pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GyBWGgY_V5wG6DoGD96txV2z5BtEfKDY

# Sintaxe básica do Pytorch

Assim como o NumPy, o Pytorch é uma biblioteca de processamento vetorial/matricial/tensorial. Operações sobre os tensores do Pytorch possuem sintaxe consideravelmente parecida com operações sobre tensores do NumPy.

Para mais informações sobre tensores em PyTorch, consulte a documentação: <br> https://pytorch.org/docs/stable/tensors.html
"""

import torch
tns = torch.randn(9, 12)
tns1 = tns[0:5, 0:4]
tns2 = tns[5:, 4:]

resultado = torch.mm(tns1, tns2)
print(resultado.size())

"""## Tipos de tensores 

Você pode criar tensores do PyTorch de inúmeras formas! Vamos ver primeiro os tipos de tensores que estão ao nosso dispor. Para isso, vamos converter listas comuns do Python em tensors do PyTorch.

Note que a impressão de tensores dos tipos ```float32``` e ```int64``` não vêm acompanhadas do parâmetro de tipo ```dtype```, visto que se tratam dos tipos padrão trabalhados pelo PyTorch.MM
"""

import torch
lista = [ [1,2,3],
          [4,5,6] ]

tns = torch.Tensor(lista)
print(tns.dtype)
print(tns)

print('')
tns = torch.DoubleTensor(lista)
print(tns.dtype)
print(tns)

print('')
tns = torch.LongTensor(lista)
print(tns.dtype)
print(tns)

"""## Outras formas de instanciar tensores

### A partir de arrays Numpy
$torch.from\_numpy()$ 


"""

import numpy as np

arr = np.random.rand(3,4)
arr = arr.astype(int)
print(arr)
print(arr.dtype)

print('')
tns = torch.from_numpy(arr)
print(tns)
print(tns.dtype)

"""### Tensores inicializados
Essas funções recebem como parâmetro o tamanho de cada dimensão do tensor. Aqui vamos conhecer as seguintes funções:

$torch.ones()$ -> Cria um tensor preenchido com zeros.

$torch.zeros()$ -> Cria um tensor preenchido com uns.

$torch.randn()$ -> Cria um tensor preenchido com números aleatórios a partir de uma distribuição normal.
"""

tns1 = torch.ones(2, 3)
tns0 = torch.zeros(3, 5)
tnsr = torch.randn(3, 3)

print(tns1)
print(tns0)
print(tnsr)

"""### Tensor para array numpy"""

arr = tnsr.data.numpy()
print(arr)

"""## Indexação

De posse dessa informação, a indexação é feita de forma similar a arrays Numpy, através da sintaxe de colchetes ```[]```
"""

# tnsr[0,1]

print(tnsr[0:2, 2].data.numpy())
print(tnsr[0, 1].item())

"""## Operações com tensores

A função ```.item()``` utilizada anteriormente extrai o número de um tensor que possui um único valor, permitindo realizar as operações numéricas do Python. Caso o item não seja extraído, operações que envolvam tensores vão retornar novos tensores.

Vale ressaltar também que operações entre tensores são realizadas **ponto a ponto**, operando cada elemento ```(i, j)``` do tensor ```t1```, com o elemento ```(i, j)``` do tensor ```t2```.
"""

tns1 = torch.randn(2,2,3)
tns2 = torch.ones(2,2,3)

print(tns1)
print(tns2)

print('')

print(tns1*tns2)

"""## Função ```.size()``` e ```.view()```

Uma operações **importantíssima** na manipulação de tensores para Deep Learning é a reorganização das suas dimensões. Dessa forma podemos, por exemplo, **linearizar um tensor n-dimensional**.
"""

print(tns2.size())

print(tns2.view(tns2.size(0), -1))

"""## GPU Cast

Para que o seu script dê suporte a infraestruturas com e sem GPU, é importante definir o dispositivo no início do seu código de acordo com a verificação apresentada a seguir. Essa definição de dispositivo será utilizada toda vez que precisarmos subir valores na GPU, como os pesos da rede, os gradientes, etc.
"""

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')
  
print(device)
tns2 = tns2.to(device)
print(tns2)

"""# Classificação Linear

Nesse notebook vamos refrescar a sua memória sobre equações lineares ao mesmo tempo que te ajudaremos a enxergar uma reta como um classificador linear. 

**Nota**: Mesmo que você já saiba tudo sobre retas, não pule essa aula! Aqui já damos os primeiros passos para a criação de redes neurais.

## Equação da reta

A equação da reta é aprendida na forma $ax + by + c$, mas para nos adequarmos às nomenclaturas de redes neurais, podemos reescrever essa equação como $\mathbf{w_1x_1 + w_2x_2 + b}$. Ou seja, $\mathbf{w_1}$, $\mathbf{w_2}$ e $\mathbf{b}$ são os parâmetros que definem o nosso **modelo linear**, a nossa reta.

Vamos começar realizando o plot de uma reta específica:
$\mathbf{-1x_1 + 4x_2 + 0.4}$
"""

import numpy as np
import matplotlib.pyplot as plt

a = -1
b = 4
c = 0.4

# ax + by + c = 0
# y = (-a*x -c)/b
def plotline(a, b, c):
  x = np.linspace(-2, 4, 50)
  y = (-a*x -c)/b

  plt.axvline(0, -1, 1, color='k', linewidth=1)
  plt.axhline(0, -2, 4, color='k', linewidth=1)
  plt.plot(x, y)
  plt.grid(True)
  
plotline(a, b, c)

"""### Solucionando a equação da reta para diferentes pontos

A seguir vamos selecionar três pontos no espaço, e soucionar a equação da reta para esses pontos. 
"""

p1 = (2, 0.4)
p2 = (1, 0.6)
p3 = (3, -0.4)

plotline(a,b,c)
plt.plot(p1[0], p1[1], color='b', marker='o')
plt.plot(p2[0], p2[1], color='r', marker='o')
plt.plot(p3[0], p3[1], color='g', marker='o')

ret1 = a*p1[0] + b*p1[1] + c
print("%.2f" % ret1)

ret2 = a*p2[0] + b*p2[1] + c
print("%.2f" % ret2)

ret3 = a*p3[0] + b*p3[1] + c
print("%.2f" % ret3)

"""Note que o ponto <font color='blue'>$p_1$</font> está na reta, enquanto <font color='red'>$p_2$</font> está acima da reta e <font color='green'>$p_3$</font> abaixo. Ao solucionar a equação da reta para esses três pontos, tivemos respectivamente resultados <font color='blue'>nulo $(=0)$</font>, <font color='red'>positivo $(>0)$</font> e <font color='green'>negativo $(<0)$</font>.

Esse comportamento é consistente para quaisquer pontos na reta, acima ou abaixo dela. Ou seja, se chamarmos a equação da reta de $f(x)$, temos as seguintes regras:

* <font color='blue'>$f(x) = 0$</font> define pontos na reta
* <font color='red'>$f(x) > 0$</font> define pontos acima da reta
* <font color='green'>$f(x) < 0$</font> define pontos abaixo da reta

## Modelo Linear

Nossa reta pode ser interpretada como um modelo linear, que nada mais é do que uma **função de mapeamento** $X \rightarrow Y$, que mapeia cada ponto em $X$ para um ponto em $Y$. Em outras palavras, dados os parâmetros $W = \{w_1, w_2\}$ e $b$ de uma reta, é possível mapear uma entrada $X = \{x_1, x_2\}$ para a saída $f(x; W, b)$.

Para problemas de classificação, os valores de $y$ para novas entradas $x$ vão definir se $x$ é um ponto <font color='red'>acima</font> ou <font color='green'>abaixo</font> da reta, formando portanto um classificador capaz de **separar linearmente problemas com duas classes**.

<img src="https://drive.google.com/uc?export=view&id=18Emd5S0LxY_-6_oqOW_1oBztomJGuRfF" width="500">


### Regra Geral

Formalizando a nomenclatura de redes neurais, em duas dimensões $(x_1, x_2)$ nosso modelo linear tem dois **pesos** $(w_1, w_2)$ e um **viés** $b$, que em inglês são chamados de **w**eight e **b**ias. 

Em duas dimensões nosso modelo forma uma reta, como vimos anteriormente. Para um número de dimensões $d > 2$, modelos lineares são chamados de **hiperplanos**, e são compostos por:
* um peso $w_i$ para cada dimensão $x_i$. 
* um único viés $b$

Exemplo, para 3 dimensões $(x_1, x_2, x_3)$ teríamos três pesos $(w_1, w_2, w_3)$ e um único viés $b$. Sua função de mapeamento seria $y = w_1x_1 + w_2x_2 + w_3x_3 + b$
 
De forma geral, define-se que dada uma entrada com $d$ dimensões, a função de mapeamento de um modelo linear é $$y = \sum^{d}_{i=1}w_ix_i + b$$

## Exercício Sugerido

**Vamos treinar um classificador linear em duas dimensões usando a tecnologia mais avançada que existe: o seu cérebro!**

A célula a seguir produz uma distribuição aleatória para um problema de classificação com duas classes, usando a função ```make_classification()``` do sklearn. <br>
Documentação: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html
"""

from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(46)

X, Y = make_classification(n_features=2, n_redundant=0, n_informative=1,
                             n_clusters_per_class=1)

plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y,
           edgecolor='k')

p = X[10]
print(Y[10])
plt.plot(p[0], p[1], marker='^', markersize=20)

"""**Seu trabalho é** encontrar os melhores parâmetros $(W, b)$ de um modelo linear para classificar a distribuição gerada. 

O ideal é **não** fazer por tentativa e erro, mas prestar atenção no impacto de cada mudança realizada. Note que os pesos $W$ são componentes angulares do seu modelo, enquanto o viés $b$ é o componente linear que desloca a reta no espaço.

"""

def plotmodel(w1, w2, b):
  
  plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y,
           edgecolor='k')
  
  xmin, xmax = plt.gca().get_xlim()
  ymin, ymax = plt.gca().get_ylim()
  
  x = np.linspace(-2, 4, 50)
  y = (-w1*x -b)/w2

  plt.axvline(0, -1, 1, color='k', linewidth=1)
  plt.axhline(0, -2, 4, color='k', linewidth=1)
  plt.plot(x, y, label='_nolegend_')
  
  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)
  
w1 = 5 #a
w2 = 1  #b
b  = -0.4  #c
plotmodel(w1, w2, b)

p = (-1, 1)
print(w1 * p[0] + w2 * p[1] + b)

"""Agora podemos utilizar o nosso modelo como um classificador linear com um simples **```if```** para avaliar a solução da equação da reta para novos pontos no espaço."""

def classify(ponto, w1, w2, b):
  ret = w1 * ponto[0] + w2 * ponto[1] + b
  
  if ret >= 0:
    return 1, 'yellow'
  else:
    return 0, 'blue'
  
p = (2, -1)
classe, cor = classify(p, w1, w2, b)
print(classe, cor)

plotmodel(w1,w2, b)
plt.plot(p[0], p[1], marker='^', color=cor, markersize=30)

"""De posse da função ```classify()``` podemos agora classificar cada um dos pontos gerados aleatoriamente pelo ```make_classification```, calculando assim a acurácia do nosso modelo."""

acertos = 0
for k in range(len(X)):
  categ, _ = classify(X[k], w1, w2, b)
  if categ == Y[k]:
    acertos += 1
    
    
print("Acurácia: {0}".format(100*acertos/len(X)))

"""# Camada ```Linear```

Vamos começar importando o módulo ```nn``` e o ```torch```
"""

import torch
from torch import nn # neural networks

"""Agora vamos instanciar nossa camada linear, como vimos nos slides, com uma entrada de dimensionalidade ```d = 3``` e a saída de tamanho ```1```, já que estamos instanciando um perceptron simples. 

Qualquer dúvida, a documentação está logo ali ;)<br>
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
"""

torch.manual_seed(42)

perceptron = nn.Linear(3, 1)
print(perceptron)

"""Temos em mãos exatamente o mesmo modelo da imagem clássica do perceptron!

<img src="https://drive.google.com/uc?export=view&id=1Ad1nKUG8ujVWysOauKYzOhhmR26Zl6yC" width="330">

Os pesos ```W``` e o viés ```b``` são inicializados aleatoriamente pelo pytorch. Podemos consultar essa informação de duas formas:
* iterando em seus **```named_parameters```**
* acessando cada parâmetro individualmente através do seu nome padrão.
"""

for nome, tensor in perceptron.named_parameters():
  print(nome, tensor.data)
  
print('')
print(perceptron.weight.data)
print(perceptron.bias.data)

w1, w2, w3 = perceptron.weight.data.numpy()[0]
b = perceptron.bias.data.numpy()

# w1 * x1 + w2 * x2 + w3 * x3 + b = 0

"""É importante não se esquecer que no perceptron esses valores definem um hiperplano no espaço. Como 3 dimensões está no limite da nossa capacidade de visualização, vamos aproveitar esse momento para visualizar o plano aleatoriamente inicializado pelo ```torch```. 

Para isso, usaremos aqui a biblioteca de visualização matplotlib, com o toolkit de plot 3D.<br>
Mais informações em: https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#mpl_toolkits.mplot3d.Axes3D.plot_surface

Para se inspirar: https://stackoverflow.com/questions/48335279/given-general-3d-plane-equation-how-can-i-plot-this-in-python-matplotlib
"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def plot3d(perceptron):
  
  w1, w2, w3 = perceptron.weight.data.numpy()[0]
  b = perceptron.bias.data.numpy()
  
  X1 = np.linspace(-1,1,10)
  X2 = np.linspace(-1,1,10)

  X1, X2 = np.meshgrid(X1,X2)

  X3 = (b - w1*X1 - w2*X2) / w3
  
  fig = plt.figure(figsize=(10, 8))

  ax = fig.gca(projection='3d')
  ax.view_init(azim=180)
  
  ax.plot_surface(X1, X2, X3, cmap='plasma')

plot3d(perceptron)

"""## Forward
O fluxo da informação de entrada através da rede recebe o nome de *forward*, já que a informação é passada *para a frente*. Vamos então realizar o forward da nossa rede de um ~~homem~~ pereptron só. Para isso, basta realizar uma chamada de função com a variável onde instaciamos a camada ```Linear```.

```python
# Fizemos anteriormente a instância da camada
linear = nn.Linear(in_features=3, out_features=1)

# Para realizar o forward
y = linear(X)
```
"""

X = torch.Tensor([0, 1, 2])
y = perceptron(X)

print(y)

plot3d(perceptron)
plt.plot([X[0]], [X[1]], [X[2]], color='r', marker='^', markersize=20)

"""# Funções de Ativação

O módulo ```nn```, que utilizamos para criar uma camada linear, também contém as principais funções de ativação. Então vamos primeiro importar o ```torch``` e o módulo ```nn``` <br>
"""

import torch
from torch import nn

"""Agora vamos retomar o exemplo do início desse capítulo, onde implementamos um modelo de classificação linear, definindo empiricamente os parâmetros adequados.
Como já fizemos essa parte passo a passo, podemos copiar o código do nosso script:<br>
Classificação-Linear.ipynb - https://colab.research.google.com/drive/14fL7OsRj4EGCnN2PjVUYFZievtGsq9ox
"""

from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(46)

X, Y = make_classification(n_features=2, n_redundant=0, n_informative=1,
                             n_clusters_per_class=1)

def plotmodel(w1, w2, b):
  
  plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y,
           edgecolor='k')
  
  xmin, xmax = plt.gca().get_xlim()
  ymin, ymax = plt.gca().get_ylim()
  
  x = np.linspace(-2, 4, 50)
  y = (-w1*x -b)/w2

  plt.axvline(0, -1, 1, color='k', linewidth=1)
  plt.axhline(0, -2, 4, color='k', linewidth=1)
  plt.plot(x, y, label='_nolegend_')
  
  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)
  
w1 = 5 #a
w2 = 1  #b
b  = 1.2  #c
plotmodel(w1, w2, b)

p = (-1, 1)
print(w1 * p[0] + w2 * p[1] + b)

"""Agora vamos instanciar uma camada Linear análoga à nossa reta, ou seja:
* ```in_features  = 2``` : dimensionalidade da entrada
* ```out_features = 1``` : dimensionalidade da saída

Vamos instanciar também uma ativação sigmóide para compor o nosso modelo de classificação.
Documentação: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity

Em seguida vamos substituir os pesos do perceptron com os parâmetros da nossa reta. Para isso criaremos um objeto ```Parameter```, o tipo de dado que camadas do PyTorch aceitam para representar os seus parâmetros. <br>
Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter

"""

perceptron = nn.Linear(2, 1)
activation = nn.Sigmoid()

print(perceptron.weight.data)
print(perceptron.bias.data)

perceptron.weight = nn.Parameter(torch.Tensor([[w1, w2]]))
perceptron.bias   = nn.Parameter(torch.Tensor([b]))

print(perceptron.weight.data)
print(perceptron.bias.data)

"""Temos portanto:
* uma distribuição de pontos com duas classes linearmente separáveis
* um modelo linear de classificação binária, empiricamente definido, que inclui uma função de ativação sigmóide. 

Agora vamos ver a diferença entre o resultado apresentado pelo perceptron antes e depois da função de ativação. Para isso, selecionei empiricamente alguns pontos da distribuiçã (índices 17, 21, 43 e 66) que nos ajudarão a entender o impacto da sigmóide.
"""

markers = ['^', 'v', '>', '<']
colors = ['r', 'g', 'b', 'gray']

plt.figure(figsize=(8, 6))
plotmodel(w1, w2, b)
for k, idx in enumerate([17, 21, 43, 66]):
  x = torch.Tensor(X[idx])
  
  ret = perceptron(x)
  act = activation(ret)
  
  act_limiar = 0 if ret.data < 0 else 1
  
  label = 'ret: {:5.2f}'.format(ret.data.numpy()[0]) + ' limiar: {:4.2f}'.format(act_limiar) + ' act: {:5.2f}'.format(act.data.numpy()[0])
  plt.plot(x[0], x[1], marker=markers[k], color=colors[k], markersize=10, label=label)
  
plt.legend()
plt.show()

"""Com a sigmoid, elementos na fronteira de decisão recebem o valor 0.5, indicando baixa certeza sobre a classe desses elementos. Quanto mais afasta da zona de confusão, maior a certeza da classificação (seja para 0, ou para 1).

Podemos experimentar diferentes funções de ativação e ver como elas nos ajudam a interpretar a saída do perceptron.
"""

# Descomente uma das linhas abaixo e rode novamente a célula anterior

activation = nn.ReLU()
# activation = nn.Tanh()

"""# Definindo uma Arquitetura no PyTorch

**Carregando dados**

Exemplo de dados não-linearmente separáveis (distribuição em lua): <br>
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html
"""

from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X1, Y1 = make_moons(n_samples=300, noise=0.2)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', 
 c=Y1, s=25, edgecolor='k')

"""## nn.Sequential

O módulo ```nn.Sequential``` é um container onde se pode colocar múltiplos módulos. Ao realizar um ```forward``` em um objeto ```Sequential``` ele aplicará sequencialmente os módulos nele contidas para gerar uma saída.

Segue abaixo um exemplo desse módulo contendo 2 camadas ```Linear``` intercaladas por uma função de ativação ```ReLU```.
"""

import torch
from torch import nn # neural networks

input_size = 2
hidden_size = 8
output_size = 1

net = nn.Sequential(nn.Linear(in_features=input_size, out_features=hidden_size), # hidden (escondida)
                    nn.ReLU(),   # ativação não linear
                    nn.Linear(in_features=hidden_size, out_features=output_size)) # output (saída)
print(net)

"""O módulo ```summary``` da biblioteca **```torchsummary```** nos permite visualizar mais informações sobre a nossa rede, como quantidade de parâmetros e o tamanho que cada elemento ocupa na memória.

Para mais informações, acesse: https://github.com/sksq96/pytorch-summary
"""

from torchsummary import summary
summary(net, input_size=(1, input_size))

"""### Forward"""

print(X1.shape)
tensor = torch.from_numpy(X1).float()
pred   = net(tensor)
print(pred.size())

"""## Classe nn.Module

A forma mais organizada de definir modelos em PyTorch é implementando uma classe nos moldes da classe ```nn.Module```. Para redes pequenas, como as que estamos aprendendo até o momento, sua importância pode não se destacar, mas modelos maiores e com funcionalidades mais complexas, são mais fáceis de implementar e realizar manutenções dessa forma.

Funções obrigatórias do ```nn.Module```.
* ```__init()__```: definição de hiperparâmetros e instância do modelo 
* ```forward()```: Fluxo da entrada para produzir uma saída
"""

class MinhaRede(nn.Module):
  
  def __init__(self, input_size, hidden_size, output_size):
    super(MinhaRede, self).__init__()
    
    # Definir a arquitetura
    self.hidden = nn.Linear(input_size, hidden_size)
    self.relu   = nn.ReLU()
    self.output = nn.Linear(hidden_size, output_size)
    
  def forward(self, X):
    
    # Gerar uma saída a partir do X
    hidden = self.relu(self.hidden(X))
    output = self.output(hidden)
    
    return output

"""Instanciando a rede"""

input_size = 2
hidden_size = 8
output_size = 1

net = MinhaRede(input_size,hidden_size, output_size)  ## O método __init__()
print(net)

"""### Forward"""

print(X1.shape)
tensor = torch.from_numpy(X1).float()
pred   = net(tensor)
print(pred.size())

"""## Subindo informações na GPU

Para conseguir executar modelos maiores em tempo hábil, é preciso carregar as informação na GPU para que o processamento seja realizado por ela. Dentre as informações que conhecemos até o momento, duas delas precisam ser carregadas ba GPU:
* Entrada
* Rede

A célula a seguir contém a implementação sugerida para realizar esse processo. <br>
Se certifique que o notebook está na configuração desejada (CPU/GPU). Para isso, basta acessar ```Edit -> Notebook Settings``` e definir o hardware desejado.
"""

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')
  
print(device)

input_size = 2
hidden_size = 8
output_size = 1

net = MinhaRede(input_size,hidden_size, output_size)  ## O método __init__()
net = net.to(device)
print(net)

print(X1.shape)
tensor = torch.from_numpy(X1).float()
tensor = tensor.to(device)
pred   = net(tensor)
print(pred.size())