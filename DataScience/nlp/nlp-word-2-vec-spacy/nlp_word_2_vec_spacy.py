# -*- coding: utf-8 -*-
"""nlp-word-2-vec-spacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5yXPfvusTnzRGBs5YpbThyMjgcslOiR

# Download Spacy e Arquivos
"""

!python -m spacy download pt_core_news_sm

!curl  http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s300.zip -O > cbow_s300.zip

!curl  http://143.107.183.175:22980/download.php?file=embeddings/word2vec/skip_s300.zip -O  > skip_s300.zip

!unzip skip_s300.zip
!unzip cbow_s300.zip

"""## Spacy e Import """

import pandas as pd
import spacy

nlp = spacy.load("pt_core_news_sm")

uri_treino = 'https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/treino.csv'
uri_teste = 'https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/teste.csv'
dados_treino = pd.read_csv(uri_treino)
dados_teste = pd.read_csv(uri_teste)
dados_treino.sample(5)

"""# Gerando Modelo"""

texto = "Rio de Janeiro é uma cidade maravilhosa"
doc = nlp(texto)

doc

type(doc[2])

textos_para_tratamento = (titulos.lower() for titulos in dados_treino["title"])

def trata_textos(doc):
    tokens_validos = []
    for token in doc:
        e_valido = not token.is_stop and token.is_alpha
        if e_valido:
            tokens_validos.append(token.text)

    if len(tokens_validos) > 2:
        return  " ".join(tokens_validos)

texto = "Rio de Janeiro 1231231 ***** @#$ é uma cidade maravilhosa!"
doc = nlp(texto)

texto = "Rio de Janeiro 1231231 ***** @#$ é uma cidade maravilhosa!"
doc = nlp(texto)
trata_textos(doc)

from time import time

t0 = time()
textos_tratados = [trata_textos(doc) for doc in nlp.pipe(textos_para_tratamento,
                                                        batch_size = 1000,
                                                        n_process = -1)]

tf = time() - t0

print(tf/60)

titulos_tratados = pd.DataFrame({"titulo": textos_tratados})
titulos_tratados.head()

from gensim.models import Word2Vec

w2v_modelo = Word2Vec(sg = 0,
                      window = 2,
                      size = 300,
                      min_count = 5,
                      alpha = 0.03,
                      min_alpha = 0.007)

w2v_modelo

print(len(titulos_tratados))
titulos_tratados = titulos_tratados.dropna().drop_duplicates()
print(len(titulos_tratados))

lista_lista_tokens = [titulo.split(" ") for titulo in titulos_tratados.titulo]

import logging

logging.basicConfig(format="%(asctime)s : - %(message)s", level = logging.INFO)

w2v_modelo = Word2Vec(sg = 0,
                      window = 2,
                      size = 300,
                      min_count = 5,
                      alpha = 0.03,
                      min_alpha = 0.007)

w2v_modelo.build_vocab(lista_lista_tokens, progress_per=5000)

dir(w2v_modelo)

w2v_modelo.corpus_count

w2v_modelo.train(lista_lista_tokens, 
                 total_examples=w2v_modelo.corpus_count,
                 epochs = 30)

w2v_modelo.wv.most_similar("google")

w2v_modelo.wv.most_similar("microsoft")

w2v_modelo.wv.most_similar("barcelona")

w2v_modelo.wv.most_similar("messi")

w2v_modelo.wv.most_similar("gm")

#Treinamento do modelo Skip-Gram
w2v_modelo_sg = Word2Vec(sg = 1,
                      window = 5,
                      size = 300,
                      min_count = 5,
                      alpha = 0.03,
                      min_alpha = 0.007)

w2v_modelo_sg.build_vocab(lista_lista_tokens, progress_per=5000)

w2v_modelo_sg.train(lista_lista_tokens, 
                 total_examples=w2v_modelo_sg.corpus_count,
                 epochs = 30)

w2v_modelo_sg.wv.most_similar("google")

w2v_modelo.wv.most_similar("google")

w2v_modelo_sg.wv.most_similar("gm")

w2v_modelo.wv.most_similar("gm")

w2v_modelo.wv.save_word2vec_format("modelo_cbow.txt", binary=False)
w2v_modelo_sg.wv.save_word2vec_format("modelo_skipgram.txt", binary=False)

"""# Vetorizando treino e teste"""

import numpy as np
from gensim.models import KeyedVectors

w2v_modelo_cbow = KeyedVectors.load_word2vec_format("modelo_cbow.txt")
w2v_modelo_sg = KeyedVectors.load_word2vec_format("modelo_skipgram.txt")
artigo_treino = dados_treino
artigo_teste = dados_teste

nlp = spacy.load("pt_core_news_sm", disable=["paser", "ner", "tagger", "textcat"])

def tokenizador(texto):
    
    doc = nlp(texto)
    tokens_validos = []
    for token in doc:
        e_valido = not token.is_stop and token.is_alpha
        if e_valido:
            tokens_validos.append(token.text.lower())

    
    return  tokens_validos

texto = "Rio de Janeiro 1231231 ***** @#$ é uma cidade maravilhosa!"
tokens = tokenizador(texto)
print(tokens)

def combinacao_de_vetores_por_soma(palavras, modelo):

    vetor_resultante = np.zeros((1,300))

    for pn in palavras:
        try:
            vetor_resultante += modelo.get_vector(pn)

        except KeyError:
            pass
                

    return vetor_resultante

vetor_texto = combinacao_de_vetores_por_soma(tokens, w2v_modelo_cbow)
print(vetor_texto.shape)
print(vetor_texto)

def matriz_vetores(textos, modelo):
    x = len(textos)
    y = 300
    matriz = np.zeros((x,y))

    for i in range(x):
        palavras = tokenizador(textos.iloc[i])
        matriz[i] = combinacao_de_vetores_por_soma(palavras, modelo)

    return matriz

matriz_vetores_treino_cbow = matriz_vetores(artigo_treino.title, w2v_modelo_cbow)
matriz_vetores_teste_cbow = matriz_vetores(artigo_teste.title, w2v_modelo_cbow)
print(matriz_vetores_treino_cbow.shape)
print(matriz_vetores_teste_cbow.shape)

"""# Treino e integração classificador"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

def classificador(modelo, x_treino, y_treino, x_teste, y_teste):

    RL = LogisticRegression(max_iter = 800)
    RL.fit(x_treino, y_treino)
    categorias = RL.predict(x_teste)
    resultados = classification_report(y_teste, categorias)
    print(resultados)
    
    return RL

RL_cbow = classificador(w2v_modelo_cbow,
                        matriz_vetores_treino_cbow,
                        artigo_treino.category,
                        matriz_vetores_teste_cbow,
                        artigo_teste.category)

matriz_vetores_treino_sg = matriz_vetores(artigo_treino.title, w2v_modelo_sg)
matriz_vetores_teste_sg = matriz_vetores(artigo_teste.title, w2v_modelo_sg)

RL_sg = classificador(w2v_modelo_sg,
                        matriz_vetores_treino_sg,
                        artigo_treino.category,
                        matriz_vetores_teste_sg,
                        artigo_teste.category)

import pickle

with open("rl_cbow.pkl", "wb") as f:
    pickle.dump(RL_cbow, f)

with open("rl_sg.pkl", "wb") as f:
    pickle.dump(RL_sg, f)

