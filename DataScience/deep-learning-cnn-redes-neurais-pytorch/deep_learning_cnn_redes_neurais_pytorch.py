# -*- coding: utf-8 -*-
"""deep-learning-cnn-redes-neurais-pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hFbshUHDVL0yX8ZWbTHZAg8VZxjFwpY2
"""

import matplotlib.pyplot as plt
from matplotlib import patches
import numpy as np

import torch 
from torchvision import datasets, transforms

"""## Classificação

Vamos usar como exemplo o dataset MNIST de dígitos escritos a mão. É um excelente dataset para quem está aprendendo técnicas de reconhecimento de padrões em imagens, como descreve o próprio autor:<br>
http://yann.lecun.com/exdb/mnist/

O MNIST possui **10 classes**, os dígitos entre 0 e 9, como apresentado na imagem a seguir.

![](https://learnmachinelearning.files.wordpress.com/2017/08/mnist.png)

Ele faz parte dos datasets disponíveis através da biblioteca ```torchvision```<br>
https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.MNIST

Na padronização do Torchvision, podemos carregar o dataset com uma simples chamada de função. Cada elemento do MNIST é representado na forma ```(dado, rótulo)```. 
"""

MNIST = datasets.MNIST('./',
                      train=False,
                      download=True,
                      transform=transforms.ToTensor())


dado, rotulo = MNIST[0]
print(type(dado), type(rotulo))

# Channel First: Padrão do torch
print(dado.size()) # C x H x W

fig, axs = plt.subplots(1,10, figsize=(22, 4))
for i in range(10):
  dado, rotulo = MNIST[i]
  axs[i].imshow(dado[0], cmap='gray')
  axs[i].set_title('Rótulo: ' + str(rotulo))
plt.show()

"""> **Como deve ser a última camada de uma rede cujo objetivo é classificar os dados do MNIST?**
---

## Detecção

Para ilustrar o problema de detecção, usaremos o dataset PascalVOC 2012:<br> 
http://host.robots.ox.ac.uk/pascal/VOC/voc2012/


O [PASCAL Visual Object Classes](http://host.robots.ox.ac.uk/pascal/VOC/) é um projeto que realiza competições para desenvolvedores que se interessam pelos desafios do reconhecimento de padrões em imagens. Dentre os desafios estão: detecção, segmentação, reconhecimento de ações, entre outros. <br>


Os dados de detecção podem ser encontrado no pacote Torchvision:<br>
https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.VOCDetection
"""

coco = datasets.VOCDetection('./', 
                             image_set='train', 
                             download=True, 
                             transform=transforms.ToTensor())

dado, rotulo = coco[0]
print(type(dado), type(rotulo))

"""Imagens coloridas possuem **canais** de cor (RGB: *red, green blue*). <br>
Por padrão o PyTorch adota o formato *channel first*, ou seja **canal primeiro**, fazendo referência à dimensionalidade da imagem: **$C \times H \times W$**<br>

Já bilbiotecas de visualização, como o matplotlib, representam as imagens com o canal na última dimensão, por isso antes de plotar a imagem, precisamos organizar as dimensões. 
"""

print('Dimensionalidade:', dado.size())
dado, rotulo = coco[0]
dado = dado.permute(1, 2, 0)

plt.figure(figsize=(8, 7) )
plt.imshow(dado)
plt.show()

rotulo

"""Estamos interessados no **bounding box** do rótulo, que numa tradução livre seria a "caixa delimitadora", um retângulo que define a localização do objeto na imagem. De acordo com o padrão de anotação do PascalVOC, o rótulo de um objeto pode ser acessado da seguinte forma:
```python
xmax, xmin, ymax, ymin = rotulo['annotation']['object'][0]['bndbox'].values()
```

Para visualizar o bounding box, usaremos os ```patches``` do matplotlib
```
rect = patches.Rectangle((xmin, ymin),w,h)
```
"""

bbox = rotulo['annotation']['object'][0]['bndbox']
xmax = int(bbox['xmax']) 
xmin = int(bbox['xmin']) 
ymax = int(bbox['ymax'])
ymin = int(bbox['ymin'])

fig, ax = plt.subplots(figsize=(8, 7))
plt.imshow(dado)

rect = patches.Rectangle((xmin, ymin), xmax-xmin , ymax-ymin, fill=False, linewidth=5, edgecolor='r')
ax.add_patch(rect)

plt.show()

fig, axs = plt.subplots(1, 5, figsize=(18, 7))
for i in range(5):
  dado, rotulo = coco[i]
  dado = dado.permute(1, 2, 0)

  bbox = rotulo['annotation']['object'][1]['bndbox']
  xmax = int(bbox['xmax']) 
  xmin = int(bbox['xmin']) 
  ymax = int(bbox['ymax'])
  ymin = int(bbox['ymin'])

  axs[i].imshow(dado)

  rect = patches.Rectangle((xmin, ymin), xmax-xmin , ymax-ymin, fill=False, linewidth=5, edgecolor='r')
  axs[i].add_patch(rect)

"""> **Como deve ser a última camada de uma rede cujo objetivo é detectar os objetos do PascalVOC?** <br>
---
"""



"""## Segmentação

O PascalVOC também fornece rótulos de segmentação para algumas de suas imagens, então continuaremos utilizando este dataset. <br>
https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.VOCSegmentation

A tarefa de segmentação consiste em uma **rotulação densa**, prevendo a localização exata dos objetos na cena a nível de píxel. A imagem a seguir apresenta ambos os rótulos de detecção e segmentação para esclarecer a diferença. 

![](https://www.researchgate.net/profile/Yizhou_Yu/publication/323410937/figure/fig5/AS:644201683361796@1530601058684/The-detection-and-semantic-segmentation-results-on-Pascal-VOC-2012-test-set-the-first_W640.jpg)

Os rótulos de segmentação em geral também são imagens, representando a máscara sobre o objeto que se deseja segmentar.
"""

coco = datasets.VOCSegmentation('./', 
                             image_set='train', 
                             download=False, 
                             transform=transforms.ToTensor(),
                             target_transform=transforms.ToTensor())

dado, rotulo = coco[0]
print(type(dado), type(rotulo))

print(dado.size(), rotulo.size())

fig, axs = plt.subplots(2,1, figsize=(10, 10))
dado = dado.permute(1, 2, 0)

axs[0].imshow(dado)
axs[1].imshow(rotulo[0], cmap='gray')

"""> Em geral, as abordagens de segmentação baseada em Redes Neurais não utilizam camadas totalmente conectadas, em vez disso utilizando arquiteturas **totalmente convolucionais**. Até o final do curso, entenderemos melhor os benefícios disso."""



"""## Convolução 1D

Antes de entrarmos no domínio das imagens, vamos entender a convolução em um contexto mais simples, trabalhando com sinais de apenas 1 dimensão.

Lembrando: A convolução é o somatório do produto entre funções, sendo uma delas **invertida e deslocada**. A Convolução **1D** indica que esta função será deslocada em apenas uma dimensão.
  
---

Vamos assumir o seguinte problema:<br>
Você decidiu coletar dados do acelerômetro de um celular. O objetivo é fazer com que pessoas caminhem com o celular no bolso para analisar como o sensor responde a esse movimento. Como demonstrado [nesse trabalho](https://www.researchgate.net/publication/221296054_Impact_of_different_walking_surfaces_on_gait_identification_based_on_higher-order_statistics_of_accelerometer_data/figures?lo=1), a magnitude no sinal do acelerômetro se altera como uma espécie de **senóide ruidosa**.


A seguir vamos simular um dado semelhante e supor que **queremos localizar os intervalos crescentes nesse sinal.**
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.signal import convolve

x = np.linspace(0, 100, 100)
sin = 10 * np.sin(x) * np.random.rand(x.shape[0])

plt.figure(figsize=(12, 3))
plt.plot(x, sin)

"""Pra facilitar, a célula a seguir apresenta uma função que usa o matplotlib para imprimir os sinais 1D como um vetor de valores."""

def show(valores, title,):
  plt.figure(figsize=(len(valores), 2))
  plt.imshow(valores[np.newaxis, :], cmap='gray')
  for k, s in enumerate(valores):
    plt.text(k, 0, '{:.1f}'.format(s), fontsize=16, color='red', ha='center', va='center')
  plt.title(title, fontsize=18)
  plt.yticks([])

"""### Representação 1D

Utilizaremos um pequeno trecho do sinal para ilustrar o efeito da convolução.
"""

sinal = sin[5:15]
show(sinal, 'Sinal')

plt.figure(figsize=(10, 4))
plt.plot(sinal)

"""### Kernel

No contexto de processamento de imagens, o kernel é um **filtro convolucional**. De forma prática, é uma matriz n-dimensional que será operada com o dado através de uma convolução.

Pode-se dizer que a convolução **mede a semelhança** entre os dois sinais.

Precisamos portanto propor um kernel que simule o padrão procurado: intervalos crescentes. 
> Mas lembre-se que a convolução opera as funções após **inverter o kernel**.
"""

kernel = np.asarray([1,0,-1])
show(kernel, 'Kernel')

kernel_invertido = np.flip(kernel)
show(kernel_invertido, 'Kernel Invertido')

"""Na célula a seguir vamos imprimir o kernel **deslocando-o ao longo do sinal** para entender passo a passo da operação."""

plt.close('all')

## Vamos alterar o valor de u 
## para deslocar o kernel
u=2
deslocamento = [float('nan')] * u
kernel_deslocado = np.hstack( (deslocamento, kernel_invertido) )

show(kernel_deslocado, 'Kernel Inv. Desl.')
show(sinal, 'Sinal')

"""Agora vamos comparar os nossos cálculos com o resultado da função ```convolve``` da biblioteca ```scipy```"""

out = convolve(sinal, kernel, mode='valid')
show(out, 'Saída')

"""Pra ficar mais visual, vamos imprimir a função resultante sobreposta ao sinal original, de modo a entender melhor como essa função nos ajuda a identificar os intervalos crescentes do sinal original."""

plt.figure(figsize=(12, 4))
plt.plot(sinal, color='k', linewidth=4)
plt.imshow(out[np.newaxis, :], cmap='Reds', aspect='auto', alpha=0.8, extent=(0.5, 8.5, -10, 10))
plt.colorbar()

"""E se rodarmos no restante do sinal como uma espécie de classificação?<br>
Neste contexto específico, podemos considerar que **ativações menores que zero** indicam que não há tendência crescente naquele intervalo do sinal.
"""

out = convolve(sin[:50], kernel, mode='valid')
out[out < 0] = 0

plt.figure(figsize=(15, 4))
plt.plot(sin[:50], color='k', linewidth=4)
plt.imshow(out[np.newaxis, :], cmap='Reds', aspect='auto', alpha=0.8, extent=(0.5, 48.5, -10, 10))
plt.xlim(-0.5, 50.5)
plt.colorbar()



"""## Convolução 2D

Agora já podemos começar a trabalhar com imagens!! A convolução **2D** consiste em deslocar um kernel ao longo do dado, podendo desta vez se deslocar em duas dimensões. Igualmente, o kernel também pode possuir duas dimensões.


"""

from skimage import io, color, transform, data
from scipy.signal import convolve
import matplotlib.pyplot as plt
import numpy as np

"""Vamos assumir que buscamos **detectar bordas em imagens**. Usaremos como exemplo a imagem de uma parede de tijolos:
```python
img = data.brick()
```
"""

img = data.brick()
plt.imshow(img, cmap='Greys')

"""Para nos auxiliar daqui pra frente, vamos definir um método para imprimir os kernels como imagens."""

def show(valores, title):
  plt.figure(figsize=(len(valores), len(valores) ))
  plt.imshow(valores, cmap='gray')
  for i, line in enumerate(valores):
    for j, col in enumerate(line):
      plt.text(j, i, '{:.0f}'.format(col), fontsize=16, color='red', ha='center', va='center')
  plt.title(title)
  plt.xticks([])
  plt.yticks([])
  plt.savefig(title+'.png', format='png', dpi=100, bbox_inches='tight')

"""Vamos agora definir dois kernels diferentes:
* Kernel de bordas **verticais**
* Kernel de bordas **horizontais**

"""

kernel_v = [ [-1, 0, 1],
             [-1, 0, 1],
             [-1, 0, 1]]

show(kernel_v, 'Kernel Vertical')

kernel_h = [ [-1, -1, -1],
             [0, 0, 0],
             [1, 1, 1]]
show(kernel_h, 'Kernel Horizontal')

"""Vamos agora convoluir ambos os kernels com a parede de tijolos e observar o resultado. Cada kernel destacará partes diferentes da imagem!!

O resultado da convolução entre o dado e o kernel é chamado de **Mapa de Características** ou **Mapa de Ativação**.
"""

mapa_de_caracteristicas = convolve(img, kernel_v, mode='valid')
plt.imshow(img, cmap='Greys')

plt.figure()
plt.imshow(mapa_de_caracteristicas, cmap='Greys')

mapa_de_caracteristicas = convolve(img, kernel_h, mode='valid')
plt.figure()
plt.imshow(mapa_de_caracteristicas, cmap='Greys')

"""### Uma observação sobre as ativações

Quando definimos o kernel, ele buscará o padrão para o qual foi definido, mas vale observar melhor o seu comportamento.
Vamos imprimir o kernel original, e a sua versão invertida que é de fato operada com a imagem durante a convolução.
"""

show(kernel_v, 'Kernel')
show(np.flip(kernel_v), 'Kernel Invertido')

"""Vamos aplicar esse kernel a uma imagem que talvez você conheça. A logo de uma empresa de cursos online em tecnologia e marketing digital!"""

!wget https://is5-ssl.mzstatic.com/image/thumb/Purple113/v4/a2/56/30/a2563080-dddc-6a02-9034-a461c6c02fae/AppIcon-0-1x_U007emarketing-0-0-85-220-0-10.png/1200x630wa.png

img = io.imread('1200x630wa.png')
img = color.rgb2gray(img)
img = transform.resize(img, (img.shape[0]//10, img.shape[1]//10) )

saida = convolve(img, kernel_v, 'valid')

plt.imshow(img, cmap='gray')
plt.title('Imagem')

plt.figure(figsize=(8, 5))
plt.imshow(np.abs(saida), cmap='gray')
plt.colorbar()
plt.title('Mapa de ativação')

"""Note que o valor absoluto da ativação do kernel é alto quando encontra um bom casamento para o seu padrão, mas também quando encontra o padrão oposto ao que carrega. No último caso, sua ativação tem sinal negativo."""



"""# Filtros Convolucionais

Retomando o que já aprendemos
* **Kernels** são filtros convolucionais. Na prática são arrays n-dimensionais que são operados com o dado através da convolução.
* Os **pesos do kernel**, ou seja, os elementos do array, carregam o padrão que será procurado no dado original.

Esses filtros podem ser manualmente projetados se já sabemos o padrão que queremos procurar, ou o mapa de ativação desejado. A seguir vamos conhecer alguns kernels convolucionais e a intuição por trás do padrão que eles projetam.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from skimage import io

from scipy.signal import convolve
import numpy as np

# %matplotlib inline

"""Na célula a seguir temos novamente a função utilizada na impressão dos resultados."""

def show(img, kernel, resultado, titulo):
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))
  plot = [img, kernel, resultado]
  titulos = ['Imagem', titulo, 'Mapa de Ativação']
  for k, ax in enumerate(axs):
    ax.imshow(plot[k], cmap='gray')
    ax.set_yticks([])
    ax.set_xticks([])
    ax.set_title(titulos[k])

  for i, line in enumerate(kernel):
    for j, col in enumerate(line):
      axs[1].text(j, i, '{:.2f}'.format(col), fontsize=12, color='red', ha='center', va='center')

"""Como dado de entrada usaremos a imagem de um dos instrutores da Alura, o Guilherme Silveira."""

!wget https://s3.us-east-1.amazonaws.com/jarvis-caelum/GUI.jpg

img = io.imread('GUI.jpg')
img = img[:,:,0]
plt.imshow(img, cmap='gray')

"""## Filtro da Média

Ao aplicar este filtro a uma imagem, o mapa de ativação resultante será uma versão suavizada da imagem original (mais "borrada", menos nítida). Este efeito é alcançado explorando a operação de convolução para **tirar a média dos pixels** de subregiões da imagem.

Os pesos desse kernel são definidos para replicar a operação da média, ou seja, para um filtro $3 \times 3$ com 9 pesos, temos que: <br><br>
\begin{equation}
\frac{\sum_{x=1}^{9} p(x)}{9} = \sum_{x=1}^{9} \frac{1}{9} p(x)
\end{equation}

Vamos modelar dois filtros:
* $3 \times 3$, com todos os pesos iguais a $\frac{1}{3^2}$ 

* $9 \times 9$, com todos os pesos iguais a $\frac{1}{9^2}$
"""

##### MEAN
# reference https://homepages.inf.ed.ac.uk/rbf/HIPR2/mean.htm
kernel_media = np.zeros((9,9))
kernel_media[:] = 1.0/(9**2)

resultado = convolve(img, kernel_media, mode='valid')
show(img, kernel_media, resultado, 'Kernel Média')

"""## Sobel 

Esse se parece muito com o filtro de bordas que já fizemos na aula passada, com a diferença que os vizinhos diretos do pixel central tem valor absoluto de maior intensidade (na horizontal ou na vertical, a depender da borda desejada).

> Após realizar a convolução, podemos imprimir os **valores absolutos** do mapa de ativação para visualizar as bordas independente do sinal da ativação.

"""

# reference: https://pt.wikipedia.org/wiki/Filtro_Sobel

##### SOBEL VERTICAL
sobel_v = np.zeros((3,3))
sobel_v[:, 0] = -1
sobel_v[:, 2] = 1
sobel_v[1, [0, 2]] = [-2, 2]
# print(sobel_v)

resultado = convolve(img, sobel_v, mode='valid')
show(img, sobel_v, np.abs(resultado), 'Sobel Vertical')


##### SOBEL HORIZONTAL
sobel_h = np.zeros((3,3))
sobel_h[0, :] = -1
sobel_h[2, :] = 1
sobel_h[[0, 2], 1] = [-2, 2]
print(sobel_h)

resultado = convolve(img, sobel_h, mode='valid')
show(img, sobel_h, np.abs(resultado), 'Sobel Horizontal')

"""## Laplace

Por fim, vamos ver mais um filtro utilizado para detecção de bordas, o Laplace. Diferente do Sobel, este filtro destaca igualmente bordas de diferentes direções.
"""

##### LAPLACE
#reference: https://en.wikipedia.org/wiki/Discrete_Laplace_operator | https://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm

laplace = np.ones((3,3)) * -1
laplace[1,1] = 8
print(laplace)

resultado = convolve(img, laplace, mode='valid')
show(img, laplace, --resultado, 'Laplace')



"""# Convolução e Padrões Complexos

Para entender melhor como a convolução funciona como um casamento de padrões, vamos agora definir um filtro convolucional mais complexo, mas capaz de encontrar um casamento perfeito no dado.

Primeiro vamos baixar a imagem que será usada de exemplo:
"""

!wget https://www.dropbox.com/s/0nrix9eknyybuqk/image_2007_000032.jpg?dl=0
!mv image_2007_000032.jpg?dl=0 plane.jpg

"""Vamos separar um próprio patch da imagem para usar como filtro de convolução. Para facilitar a visualização, selecionamos a região da turbina, localizada nas coordenadas ```[109, 129, 255, 275]```"""

import matplotlib.patches as patches

# Imprime imagem e seleciona retangulo
img = io.imread('plane.jpg')
img = img[:,:,0]
retangulo = [109, 129, 255, 275]


# # Imprime imagem e retangulo
fig, ax = plt.subplots()
ax.imshow(img, cmap='gray')
ax.add_patch(patches.Rectangle((retangulo[2], retangulo[0]),
                               (retangulo[3] - retangulo[2]),
                               (retangulo[1] - retangulo[0]), color='red', fill=False))

"""Para que o patch possa ser utilizado como filtro de convolução, precisamos:
* Subtrair o patch pelo valor do pixel médio (centralizando a distribuição)
* Inverter o filtro com a função ```flip``` do numpy. 
"""

patch = img[retangulo[0]:retangulo[1], retangulo[2]:retangulo[3]]
patch = patch - patch.mean()
patch = np.flip(patch)

plt.imshow(patch, cmap='gray')

"""Vamos ver agora o resultado da convolução."""

resultado = convolve(img, patch, mode='full')

fig, ax = plt.subplots(figsize=(15, 5))
ax.imshow(resultado, cmap='gray')
ax.add_patch(patches.Rectangle((retangulo[2], retangulo[0]),
                               (retangulo[3] - retangulo[2]),
                               (retangulo[1] - retangulo[0]), color='red', fill=False))

"""# Camadas Convolucional

Antes de mais nada, vamos fazer os imports.
"""

import torch
from torch import nn #neural networks

from skimage import data

import matplotlib.pyplot as plt

"""## Convolucional

Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d

Começando pela operação principal, a convolução está contida na camada ```nn.Conv2d```. Dentre os parâmetros que ela recebe, vamos focar nos que já conhecemos, que já são suficientes para uma diversa gama de aplicações. 

```python
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
```

**```in_channels```**<br>
Assim como toda camada, seu primeiro parâmetro é referente ao **tamanho da entrada**. Não é necessário explicitar as 2 primeiras dimensões da entrada, apenas a profundidade, que corresponde à **quantidade de canais**.

**```out_channels```** <br>
Independentemente do número de canais de entrada, um único filtro convolucional terá como resultado um canal de saída. Alterando esse parâmetro estamos na verdade definindo o **número de filtros** que irão compor essa camada, interferindo na quantidade de neurônios dessa camada.

> Dica: Nas arquiteturas de redes neurais mais populares, aumentamos a dimensão do canal à medida que avançamos na rede neural, geralmente diminuindo a resolução.

**```kernel_size```** <br>
Tamanho dos filtros convolucionais. Pode ser uma tupla ou um único número. Ex: ```kernel_size = 3``` criará filtros $3 \times 3$

**```stride```** <br>
Controla o pulo da convolução ao longo da imagem. 

**```padding```** <br>
Preenchimento com zeros nas bordas da imagem.

Vamos brincar um pouco com a camada convolucional para entender o seu funcionamento. Para isso, considere duas amostras de entrada retiradas do módulo ```skimage```:
* [brick](https://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.brick) : Imagem preta e branca de uma parede de tijolos
* [astronaut](https://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.astronaut) : Imagem colorida da astronauta Eileen Collins
"""

pb = data.brick()
rgb = data.astronaut()

print(pb.shape, rgb.shape)

plt.imshow(pb, cmap='Greys')
plt.figure()
plt.imshow(rgb)

"""Na célula a seguir vamos definir uma camada convolucional para realizar o forward na figura *brick*. Para isso, preste atenção em alguns detalhes:

* A entrada deve ser um dado tipo Tensor.
* A camada Convolucional espera uma entrada com as seguintes dimensões:
 $B \times C \times H \times W$
"""

conv = nn.Conv2d(in_channels=1, out_channels=16, 
                 kernel_size=3)

print(conv)
pb_tns = torch.Tensor(pb)
pb_tns = pb_tns.view(1,1, pb_tns.size(0), pb_tns.size(1))
print(pb_tns.size())

mapa_de_ativacao = conv(pb_tns)
print(mapa_de_ativacao.size()) # 1, 16, 510, 510

"""Agora vamos fazer o mesmo para a figura da astronauta. O que deve mudar em relação à célula anterior?

* Em PyTorch a dimensão dos canais deve vir primeiro
* A definição da camada deve comportar uma entrada com 3 canais.
"""

conv = nn.Conv2d(in_channels=3, out_channels=16, 
                 kernel_size=5, padding=2)

print(conv)
rgb_tns = torch.Tensor(rgb)
rgb_tns = rgb_tns.permute(2, 0, 1).unsqueeze(0)
print(rgb_tns.size())

mapa_de_ativacao = conv(rgb_tns)
print(mapa_de_ativacao.size()) # 1, 16, 510, 510

import torch
from torch import nn

from skimage import data



"""## Pooling

Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d

```python
torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)
```

**```kernel_size```** <br>
Tamanho dos *Field of View*. Pode ser uma tupla ou um único número. Ex: ```kernel_size = 3``` definirá FoV de $3 \times 3$

**```stride```** <br>
Controla o pulo da janela deslizante. 

**```padding```** <br>
Preenchimento com zeros nas bordas da imagem.

A camda de pooling espera uma entrada de **pelo menos** 3 dimensões ($C \times H \times W$), mas em geral a rede irá prover também a dimensão do batch ($B \times C \times H \times W$) 
"""

tns = torch.FloatTensor([ [ [ 1, 2,3 ], 
                            [4,5,6], 
                            [7,8,9]  ] ] )

pool = nn.MaxPool2d(2, stride=1)
saida = pool(tns)

print(tns.size())
print(tns)
print(saida.size())
print(saida)

"""Ao processar dados com múltiplos canais, a camada de pooling processa cada canal de entrada separadamente ao invés de processar todos os canais como em uma camada convolucional. Isso significa que **o número de canais de saída para a camada de pooling é o mesmo que o número de canais de entrada**. 

Vamos processar abaixo a imagem da astronauta.
"""

conv = nn.Conv2d(in_channels=3, out_channels=16, 
                 kernel_size=3, padding=1)

rgb = data.astronaut()
rgb_tns = torch.Tensor(rgb)
rgb_tns = rgb_tns.permute(2, 0, 1).unsqueeze(0)
mapa_de_ativacao = conv(rgb_tns)
print('Feature Map:', mapa_de_ativacao.shape)

pool = nn.MaxPool2d(kernel_size=2)
saida = pool(mapa_de_ativacao)
print(saida.size())



"""## Batch Normalization

Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d

```python
torch.nn.BatchNorm2d(num_features)
```

**```num_features```**<br>
$\mathbf{\gamma}$ e $\mathbf{\beta}$ são aprendidos individualmente para cada canal da entrada. Em ativações de camadas intermediárias, esse valor corresponde ao **número de feature maps**. 

"""

blococonv = nn.Sequential(
            nn.Conv2d(3,32,kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=10)
)
print(blococonv)

minibatch = torch.cat(12*[rgb_tns])

print(minibatch.size())
saida = blococonv(minibatch)
print(saida.size())

"""# Treino do zero (*from scratch*)

Neste script vamos conhecer a estratégia de treino tradicional de redes neurais em geral: **O treino do zero (*from scratch*)**. <br>
Trataremos do uso mais tradicional de CNNs: **classificação de imagens**.


Primeiro de tudo, vamos fazer os imports.
"""

# Commented out IPython magic to ensure Python compatibility.
# Implementação e treinamento da rede
import torch
from torch import nn, optim

# Carregamento de Dados
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms

from torch import optim

# Plots e análises
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np
import time, os

# %matplotlib inline

# Configurando hiperparâmetros.
args = {
    'epoch_num': 30,     # Número de épocas.
    'lr': 1e-3,           # Taxa de aprendizado.
    'weight_decay': 1e-3, # Penalidade L2 (Regularização).
    'batch_size': 50,     # Tamanho do batch.
}

# Definindo dispositivo de hardware
if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')

print(args['device'])

"""## Carregamento de Dados

Usaremos o dataset [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), um conjunto de imagens RGB divididas em 10 categorias de objeto: avião, automóvel, pássaro, gato, veado, cachorro, sapo, cavalo, navio, caminhão. As imagens possuem $32 \times 32$ pixels.

Trata-se de um dataset de 60 mil imagens naturais (do mundo real), muito utilizado para avaliar a qualidade de modelos de aprendizado profundo.

https://pytorch.org/docs/stable/torchvision/datasets.html#cifar
"""

data_transform = transforms.Compose([
                                     transforms.Resize(32),
                                     transforms.ToTensor(),])

train_set = datasets.CIFAR10('.', 
                      train=True, 
                      transform=data_transform, 
                      download=True)

test_set = datasets.CIFAR10('.', 
                      train=False, 
                      transform=data_transform, 
                      download=False)

fig, axs = plt.subplots(1,10, figsize=(20, 2))
for i in range(10):
  data, label = test_set[i]
  axs[i].imshow(data.permute((1,2,0)))
  axs[i].axis('off')

train_loader = DataLoader(train_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True)

test_loader = DataLoader(test_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True)

"""## LeNet 5

Primeiro de tudo, precisamos **implementar uma CNN**. Sim, chegou a hora de colocar em prática tudo que já sabemos sobre redes convolucionais, montando uma arquitetura completa!

A arquitetura escolhida para essa aula é a LeNet. Ela é a primeira CNN bem sucedida da história, [proposta em 1998 pelo Yann LeCun](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). 

<img src="https://drive.google.com/uc?export=view&id=1BThCsFE8fSCe012klZmp8UpslOlzWsmK" width="800">

<img src="https://drive.google.com/uc?export=view&id=1oG_Jh8nA2Nrq8RTcIyJEOaIXXxa10Srw" width="700">

Vamos implementar blocos convolucionais incluindo todas as camadas que aprendemos nesse curso:
```python
net = nn.Sequential(
          nn.Conv2d(...),
          nn.BatchNorm2d(...),
          nn.Tanh(), # Ativação específica da LeNet
          nn.AvgPool2d(...), # Pooling específico da LeNet
      )
```
"""

# Definindo a rede
net = nn.Sequential(
        ## ConvBlock 1
        nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),        # entrada: (b, 3, 32, 32) e saida: (b, 6, 28, 28)
        nn.BatchNorm2d(6),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size=2, stride=2, padding=0),           # entrada: (b, 6, 28, 28) e saida: (b, 6, 14, 14)
        
        ## ConvBlock 2
        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),       # entrada: (b, 6, 14, 14) e saida: (b, 16, 10, 10)
        nn.BatchNorm2d(16),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size=2, stride=2, padding=0),           # entrada: (b, 16, 10, 10) e saida: (b, 16, 5, 5)
        
        ## ConvBlock 3
        nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0),     # entrada: (b, 16, 5, 5) e saida: (b, 120, 1, 1)
        nn.BatchNorm2d(120),
        nn.Tanh(),
        nn.Flatten(),  # lineariza formando um vetor                # entrada: (b, 120, 1, 1) e saida: (b, 120*1*1) = (b, 120)
        
        ## DenseBlock
        nn.Linear(120, 84),                                         # entrada: (b, 120) e saida: (b, 84)
        nn.Tanh(),
        nn.Linear(84, 10),                                          # entrada: (b, 84) e saida: (b, 10)
        )

# Subindo no hardware de GPU (se disponível)
net = net.to(args['device'])

"""VGG16"""

# # Definindo a rede
# net = nn.Sequential(
#         ## ConvBlock 1
#         nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),      # entrada: (b, 3, 224, 224) e saida: (b, 64, 224, 224)
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 64, 224, 224) e saida: (b, 64, 112, 112)
        
#         ## ConvBlock 2
#         nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),    # entrada: (b, 64, 112, 112) e saida: (b, 128, 112, 112)
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 128, 112, 112) e saida: (b, 128, 56, 56)
        
#         ## ConvBlock 3
#         nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),   # entrada: (b, 128, 56, 56) e saida: (b, 256, 56, 56)
#         nn.ReLU(),
#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),   # entrada: (b, 256, 56, 56) e saida: (b, 256, 56, 56)
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 256, 56, 56) e saida: (b, 256, 28, 28)
        
#         ## ConvBlock 4
#         nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 256, 28, 28) e saida: (b, 512, 28, 28)
#         nn.ReLU(),
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 28, 28) e saida: (b, 512, 28, 28)
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 512, 28, 28) e saida: (b, 512, 14, 14)
        
#         ## ConvBlock 4
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 14, 14) e saida: (b, 512, 14, 14)
#         nn.ReLU(),
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 14, 14) e saida: (b, 512, 14, 14)
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 512, 14, 14) e saida: (b, 512, 7, 7)
#         nn.Flatten(),  # lineariza formando um vetor               # entrada: (b, 512, 7, 7) e saida: (b, 512*7*7) = (b, 25088)
 
#         ## DenseBlock
#         nn.Linear(25088, 4096),                                    # entrada: (b, 25088) e saida: (b, 4096)
#         nn.ReLU(),
#         nn.Linear(4096, 4096),                                     # entrada: (b, 4096) e saida: (b, 4096)
#         nn.ReLU(),
#         nn.Linear(4096, 10),                                       # entrada: (b, 4096) e saida: (b, 10)
#         nn.Softmax(dim=-1)
#         )

# # Subindo no hardware de GPU (se disponível)
# net = net.to(args['device'])

"""#Estratégias de Treino

## Do zero (From scratch)

O primeiro passo do treinamento do zero é definir os algoritmos que serão utilizados no processo de treinamento. Eles são:

* **Função de perda**, que vai avaliar a qualidade da performance da rede a cada passo de treinamento;
* **Otimizador**, que a partir da função de perda vai definir a melhor forma de atualizar os pesos.
"""

criterion = nn.CrossEntropyLoss().to(args['device'])
optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])

"""Relembrando o passo a passo do fluxo de treinamento:

* Iterar nas épocas
* Iterar nos batches
* Cast dos dados no dispositivo de hardware
* Forward na rede e cálculo da loss
* Zerar o gradiente do otimizador
* Cálculo do gradiente e atualização dos pesos

Para acompanhar a convergência do seu modelo (e garantir que tudo foi feito certinho), ao final de cada época podemos imprimir a média e o desvio padrão das perdas de cada iteração.
"""

def train(train_loader, net, epoch):

  # Training mode
  net.train()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  for batch in train_loader:
    
    dado, rotulo = batch
    
    # Cast do dado na GPU
    dado = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])
    
    # Forward
    ypred = net(dado)
    loss = criterion(ypred, rotulo)
    epoch_loss.append(loss.cpu().data)

    _, pred = torch.max(ypred, axis=1)
    pred_list.append(pred.cpu().numpy())
    rotulo_list.append(rotulo.cpu().numpy())
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   
  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  acc = accuracy_score(pred_list, rotulo_list)
  
  end = time.time()
  print('#################### Train ####################')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))
  
  return epoch_loss.mean()

"""### Validação

Para essa etapa, o PyTorch oferece dois artifícios:
* ```model.eval()```: Impacta no *forward* da rede, informando as camadas caso seu comportamento mude entre fluxos (ex: dropout).
* ```with torch.no_grad()```: Gerenciador de contexto que desabilita o cálculo e armazenamento de gradientes (economia de tempo e memória). Todo o código de validação deve ser executado dentro desse contexto.

Exemplo de código para validação

```python
net.eval()
with torch.no_grad():
  for batch in test_loader:
      # Código de validação
```

Existe o equivalente ao ```model.eval()``` para explicitar que a sua rede deve estar em modo de treino, é o ```model.train()```. Apesar de ser o padrão dos modelos, é boa prática definir também o modo de treinamento.
"""

def validate(test_loader, net, epoch):

  # Evaluation mode
  net.eval()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  with torch.no_grad(): 
    for batch in test_loader:

      dado, rotulo = batch

      # Cast do dado na GPU
      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Forward
      ypred = net(dado)
      loss = criterion(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

      _, pred = torch.max(ypred, axis=1)
      pred_list.append(pred.cpu().numpy())
      rotulo_list.append(rotulo.cpu().numpy())

  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  acc = accuracy_score(pred_list, rotulo_list)
  
  end = time.time()
  print('********** Validate **********')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))
  
  return epoch_loss.mean()

train_losses, test_losses = [], []
for epoch in range(args['epoch_num']):
  
  # Train
  train_losses.append(train(train_loader, net, epoch))
  
  # Validate
  test_losses.append(validate(test_loader, net, epoch))

"""# Estratégias de Treino

Neste script vamos conhecer as principais estratégias de treino e utilização de redes neurais convolucionais:
* Treino do zero (*from scratch*)
* **Extração de Características**
* ***Fine-Tuning* (ajuste fino)**

Mas antes de tudo, vamos fazer os imports.
"""

# Commented out IPython magic to ensure Python compatibility.
# Implementação e treinamento da rede
import torch
from torch import nn, optim

# Carregamento de Dados e Modelos
from torch.utils.data import DataLoader
from torchvision import datasets, models
from torchvision import transforms

# Plots e análises
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np
import time, os

# %matplotlib inline

# Configurando hiperparâmetros.
args = {
    'epoch_num': 5,      # Número de épocas.
    'lr': 1e-3,           # Taxa de aprendizado.
    'weight_decay': 8e-4, # Penalidade L2 (Regularização).
    'batch_size': 20,     # Tamanho do batch.
}

# Definindo dispositivo de hardware
if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')

print(args['device'])

"""# Extração de características

Esta estratégia tem como base o uso de modelos **pré-treinados** em datasets de larga escala. Redes treinadas em uma quantidade significativa e bem diversa de dados são capazes de extrair características de altíssima qualidade em dados que nunca viram antes.

Como apresentado na imagem, a extração é realizada da seguinte forma:
* **Adapta-se a rede**: A camada de classificação é removida da rede pré-treinada. 
* **Extrai as características**: Realiza o forward dos dados na rede. A última camada agora não realiza classificação, apenas produz características de alto nível semântico.
* **Treina um classificador comum**: Alimenta as características extraídas dos dados de treino e de teste a um classificador (como o SVM).

<img src="https://drive.google.com/uc?export=view&id=1Pulm0YqT53yB34eCkQSbCbzHuBPtAhU9" alt="drawing" width="650"/>

## Carregando Dados

Nesse script usaremos um modelo pré-treinado no **[ImageNet](http://www.image-net.org/)**, um dataset com 1.000 classes de objeto. Até os dias de hoje, modelos pré-treinados nesse conjunto de dados produzem características que servem como base para inúmeras aplicações.

O carregamento de dados deve seguir **o mesmo pré-processamento** do treinamento do modelo utilizado. <br>
Pré-processamento com base no ImageNet: https://github.com/pytorch/examples/blob/master/imagenet/main.py#L202

Usaremos uma **[composição de transformações](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose)** do PyTorch para realizar múltiplas transformações no dado.

```python
transforms.Compose([
     # Lista de transformações.
])
```
"""

data_transform = transforms.Compose([
                                     transforms.Resize(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                          std=[0.229, 0.224, 0.225])
                  ])

train_set = datasets.CIFAR10('.', 
                      train=True, 
                      transform= data_transform, # transformação composta 
                      download=True)

test_set = datasets.CIFAR10('.', 
                      train=False, 
                      transform= data_transform, # transformação composta 
                      download=False)

fig, axs = plt.subplots(1,10, figsize=(20, 2))
for i in range(10):
  data, label = test_set[i]
  axs[i].imshow(data.permute((1,2,0)))
  axs[i].axis('off')

train_loader = DataLoader(train_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True)

test_loader = DataLoader(test_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True)

"""## Adaptando a rede

Vamos escolher um modelo da biblioteca de modelos pré-treinados do PyTorch:<br>
https://pytorch.org/docs/stable/torchvision/models.html

Infelizmente o PyTorch não inclui um modelo pré-treinado da LeNet que implementamos anteriormente. Usaremos então um dos modelos modernos de CNN.
"""

net = models.vgg16_bn(pretrained=True).to(args['device'])
print(net)

"""Através da função ```net.children()``` conseguimos acessar individualmente os módulos implementados na rede. 

https://pytorch.org/docs/stable/nn.html#torch.nn.Module.children
"""

print(list(net.named_children())[-1])

"""Podemos redefinir a rede com todos os módulos originais **exceto o último**, responsável pela classificação das características aprendidas pelas camadas anteriores."""

net.classifier = nn.Sequential(list(net.children())[-1][:-3]).to(args['device'])
print(net)

"""## Extraindo

Nesse passo realizamos o mesmo fluxo da função de validação que implementamos anteriormente. Dessa vez não precisamos calcular a predição final, apenas armazenar as saídas da rede.
"""

def extrai_caracteristicas(net, loader):

  # Evaluation mode
  net.eval()

  feat_list, rotulo_list = [], []
  with torch.no_grad(): 
    for k, batch in enumerate(loader):
      print('\r--{0}/{1}--'.format(k, len(loader)), end='', flush=True)

      dado, rotulo = batch

      # Cast do dado na GPU
      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Extração
      caracteristica = net(dado)
      feat_list.append(caracteristica.detach().cpu().numpy())
      rotulo_list.append(rotulo.detach().cpu().numpy())

  feat_list    = np.asarray(feat_list)
  feat_list    = np.reshape(feat_list, (feat_list.shape[0]*feat_list.shape[1], feat_list.shape[2]))

  rotulo_list  = np.asarray(rotulo_list).ravel()
 
  return feat_list, rotulo_list

print('Load train')
train_X, train_Y = extrai_caracteristicas(net, train_loader)
print('\nLoad test')
test_X,  test_Y  = extrai_caracteristicas(net, test_loader)

"""### Treinando um modelo de classificação (SVM)
Documentação: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html

Modelos de classificação no pacote ```scikit-learn``` seguem o seguinte padrão de uso:
```python
# Definição do classificador
clf = LinearSVC()
#Treinando 
clf.fit(X, y)
# Usando no teste
pred = clf.predict(Xt)
```
"""

from sklearn.svm import LinearSVC

clf = LinearSVC()
clf.fit(train_X, train_Y)

pred = clf.predict(test_X)
print('{:.2f}%'.format(accuracy_score(test_Y, pred)*100))

"""---


# Fine-Tuning

O objetivo aqui é aproveitar o valor de um modelo bem treinado e sua capacidade de generalizar pra dados que nunca viu antes.
 
Lembre-se que o aprendizado é hierárquico, ou seja, camadas iniciais (em vermelho na imagem) aprendem características de baixo nível (bordas, quinas, etc.) que podem ser aproveitadas para múltiplos datasets. **Adaptamos então camadas mais semânticas** (em amarelo na imagem), que aprendem características mais específicas do dataset de treino. 

Aqui seguiremos dois passos:
* Substituir a camada de classificação da rede original;
* "Congelar" camadas iniciais da rede, para preservar seus pesos;
* Definir múltiplas taxas de aprendizado para treinar as camadas finais.

<img src="https://drive.google.com/uc?export=view&id=1BHzTprOsXunB7xmKZ7amcScfh2MRSqO1" alt="drawing" width="400"/>

## Adaptando a Rede
"""

net = models.vgg16_bn(pretrained=True).to(args['device'])
print(net)

"""Dessa vez vamos redefinir a camada de classificação, criando uma nova camada de dimensões adequadas ao nosso desafio.

* O modelo original classificava as 1.000 classes do ImageNet <br>
```nn.Linear(in_features, 1000)```
* Nossa adaptação irá classificar 10 classes do CIFAR10 <br> 
```nn.Linear(in_features, 10)```
"""

in_features = list(net.children())[-1][-1].in_features

new_classifier =list(net.classifier.children())[:-1]
new_classifier.append(nn.Linear(in_features, 10))

net.classifier = nn.Sequential(*new_classifier).to(args['device'])
print(net.classifier)

"""## Definindo múltiplas taxas de aprendizado

Para "congelar" as camadas iniciais basta alterar o atributo ```requires_grad```, definindo-o como ```False``` para parâmetros que não queremos treinar. Podemos iterar nos parâmetros da rede através da função ```net.named_parameters()```.

Para definir múltiplas taxas de aprendizados no otimizador, podemos consultar a documentação do pacote ```optim```:<br> 
https://pytorch.org/docs/stable/optim.html

Podemos definir uma lista de dicionários para cada módulo que desejamos treinar.
```python
optim.Adam([
            {'params': model.base.parameters()},
            {'params': model.classifier.parameters(), 'lr': 1e-3}
          ], lr=0)
```
"""

optimizer = optim.Adam([
            {'params': net.features.parameters(), 'lr':args['lr']*0.2, 'weight_decay': args['weight_decay']*0.2},
            {'params': net.classifier.parameters(), 'lr': args['lr'], 'weight_decay': args['weight_decay']}
        ], lr=0)

"""Não esquece de definir a função de perda :)"""

criterion = nn.CrossEntropyLoss().to(args['device'])

"""## Treino e Validação

Aqui usaremos o fluxo idêntico ao que implementamos anteriormente. Já alteramos a rede e o otimizador para indicar que realizaremos um fine-tuning e não um treinamento *from scratch*.
"""

def train(train_loader, net, epoch):

  # Training mode
  net.train()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  for k, batch in enumerate(train_loader):
    print('\r--{0}/{1}--'.format(k, len(train_loader)), end='', flush=True)
    dado, rotulo = batch
    
    # Cast do dado na GPU
    dado = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])
    
    # Forward
    ypred = net(dado)
    # print(ypred.size(), rotulo.size())
    loss = criterion(ypred, rotulo)
    # print(loss.size())
    epoch_loss.append(loss.cpu().data)

    _, pred = torch.max(ypred, axis=1)
    pred_list.append(pred.cpu().numpy())
    rotulo_list.append(rotulo.cpu().numpy())
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   
  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  acc = accuracy_score(pred_list, rotulo_list)
  
  end = time.time()
  print('\n#################### Train ####################')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))
  
  return epoch_loss.mean()

def validate(test_loader, net, epoch):

  # Evaluation mode
  net.eval()
  
  start = time.time()
  
  epoch_loss  = []
  pred_list, rotulo_list = [], []
  with torch.no_grad(): 
    for k, batch in enumerate(test_loader):
      
      print('\r--{0}/{1}--'.format(k, len(test_loader)), end='', flush=True)
      dado, rotulo = batch

      # Cast do dado na GPU
      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Forward
      ypred = net(dado)
      loss = criterion(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

      _, pred = torch.max(ypred, axis=1)
      pred_list.append(pred.cpu().numpy())
      rotulo_list.append(rotulo.cpu().numpy())

  epoch_loss = np.asarray(epoch_loss)
  pred_list  = np.asarray(pred_list).ravel()
  rotulo_list  = np.asarray(rotulo_list).ravel()

  acc = accuracy_score(pred_list, rotulo_list)
  
  end = time.time()
  print('\n********** Validate **********')
  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))
  
  return epoch_loss.mean()

train_losses, test_losses = [], []
for epoch in range(args['epoch_num']):
  
  # Train
  train_losses.append(train(train_loader, net, epoch))
  
  # Validate
  test_losses.append(validate(test_loader, net, epoch))