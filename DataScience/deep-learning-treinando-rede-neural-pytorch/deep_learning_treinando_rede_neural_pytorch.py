# -*- coding: utf-8 -*-
"""deep-learning-treinando-rede-neural-pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R4k99Sz6jawCL3RM6iMIPei0DHhxgWId

# Fun√ß√µes de Perda

O m√≥dulo ```nn``` e suas 1001 utilidades, tamb√©m fornece as implementa√ß√µes das principais fun√ß√µes de perda. Ent√£o vamos primeiro importar o ```torch``` e o m√≥dulo ```nn``` <br>
"""

import torch
from torch import nn

"""Antes de tudo, vamos conferir qual dispositivo de hardware est√° dispon√≠vel para uso."""

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

print(device)

"""Vamos trabalhar com o dataset de classifica√ß√£o de vinhos.

https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html

"""

from sklearn import datasets

wine = datasets.load_wine()
data = wine.data
target = wine.target

print(data.shape, target.shape)
print(wine.feature_names, wine.target_names)

"""
Vamos instanciar um MLP com uma camada escondida e uma camada de sa√≠da. <br>"""

class WineClassifier(nn.Module):

  def __init__(self, input_size, hidden_size, out_size):
    super(WineClassifier, self).__init__()

    self.hidden  = nn.Linear(input_size, hidden_size)
    self.relu    = nn.ReLU()
    self.out     = nn.Linear(hidden_size, out_size)
    self.softmax = nn.Softmax()

  def forward(self, X):
    
    feature = self.relu(self.hidden(X))
    output  = self.softmax(self.out(feature))

    return output

input_size  = data.shape[1]
hidden_size = 32
out_size    = len(wine.target_names)

net = WineClassifier(input_size, hidden_size, out_size).to(device) #cast na GPU

print(net)

"""## Classifica√ß√£o

O primeiro passo √© instanciar a fun√ß√£o de perda de sua escolha. Trata-se de um problema de classifica√ß√£o com 3 classes, nesse caso a Cross Entropy √© a fun√ß√£o recomendada, que no PyTorch recebe o nome de *CrossEntropyLoss*: https://pytorch.org/docs/stable/nn.html#crossentropyloss 

**Assim como a rede, as entradas e os r√≥tulos, a fun√ß√£o de perda tamb√©m deve ser carregada na GPU**

"""

criterion = nn.CrossEntropyLoss().to(device) # cast na GPU

"""Antes de aplicar a fun√ß√£o de perda, vamos fazer o cast dos dados para tensores e extrair as predi√ß√µes ```y'``` da rede."""

Xtns = torch.from_numpy(data).float()
Ytns = torch.from_numpy(target)

# Cast na GPU
Xtns = Xtns.to(device)
Ytns = Ytns.to(device)

print(Xtns.dtype, Ytns.dtype)

pred = net(Xtns)

"""Confira as dimens√µes de ```y``` e ```y'```. Enquanto as predi√ß√µes est√£o em termos de probabilidades, os r√≥tulos de classifica√ß√£o devem s√£o valores inteiros referentes aos √≠ndices das classes."""

print(pred.shape, Ytns.shape)

print(pred[0].data, Ytns[0].data)

"""As fun√ß√µes de perda implementadas no PyTorch esperam o seguinte padr√£o de chamada:

```python
loss = criterion(prediction, target)
```

Vale lembrar que cada fun√ß√£o de perda possui especificidades quanto √†s dimens√µes dos seus par√¢metros. Para a Cross Entropy:
* prediction: ```(N, C)```
* target: ```(N,)```
"""

loss = criterion(pred, Ytns)
print(loss)

"""## Regress√£o

Vamos trabalhar com o dataset de Diabetes, cujo objetivo √© prever a progress√£o da diabetes em um paciente.

https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset
"""

from sklearn import datasets

diabetes = datasets.load_diabetes()
data = diabetes.data
target = diabetes.target

print(data.shape, target.shape)

print(data[14])
print(target[14])

"""Implementando o MLP"""

class WineClassifier(nn.Module):

  def __init__(self, input_size, hidden_size, out_size):
    super(WineClassifier, self).__init__()

    self.hidden  = nn.Linear(input_size, hidden_size)
    self.relu    = nn.ReLU()
    self.out     = nn.Linear(hidden_size, out_size)
    self.softmax = nn.Softmax(dim=-1)

  def forward(self, X):
    
    feature = self.relu(self.hidden(X))
    output  = self.softmax(self.out(feature))

    return output

input_size  = data.shape[1]
hidden_size = 32
out_size    = 1  # Progress√£o da diabetes

net = WineClassifier(input_size, hidden_size, out_size).to(device) #cast na GPU

"""Para solucionar problemas de regress√£o, as fun√ß√µes de perda correspondentes esperam que ambos o r√≥tulo e a predi√ß√£o tenham **a mesma dimensionalidade**. N√£o se trata mais de um problema categ√≥rico.

Portanto, vamos simular um problema de regress√£o e aplicar a *MSELoss*<br>
Documenta√ß√£o: https://pytorch.org/docs/stable/nn.html#mseloss
"""

criterion = nn.MSELoss().to(device)

# Cast na GPU
Xtns = torch.from_numpy(data).float().to(device)
Ytns = torch.from_numpy(target).float().to(device)

print(Xtns.shape, Ytns.shape)

pred = net(Xtns)

loss = criterion(pred.squeeze(), Ytns)
print(loss.data)

criterion = nn.L1Loss().to(device)

pred = net(Xtns)

loss = criterion(pred.squeeze(), Ytns)
print(loss.data)

"""## Documenta√ß√£o
Veja a documenta√ß√£o para consultar a lista de todas as fun√ß√µes de perda implementadas no PyTorch: <br>
https://pytorch.org/docs/stable/nn.html#loss-functions

# Otimiza√ß√£o

Para entender o processo de otimiza√ß√£o, vamos utilizar um dataset de classifica√ß√£o de vinhos. A classifica√ß√£o √© feita com base em an√°lises qu√≠micas realizadas em **tr√™s diferentes cultivos** na mesma regi√£o da It√°lia. 

O carregamento dos dados est√° descrito na documenta√ß√£o do Scikit-Learn:<br>
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html

Dentre os 13 atributos do dataset, selecionamos 2 para facilitar a visualiza√ß√£o dos resultados:
* Teor Alco√≥lico: √≠ndice 0 
* Intensidade da cor: √≠ndice 9
"""

from sklearn import datasets
import matplotlib.pyplot as plt

features = [0, 9]

wine = datasets.load_wine()
data = wine.data[:, features]
targets = wine.target

print(wine.feature_names)

plt.scatter(data[:, 0], data[:,1], c=targets, s=15, cmap=plt.cm.brg)
plt.xlabel(wine.feature_names[features[0]])
plt.ylabel(wine.feature_names[features[1]])

"""### Padroniza√ß√£o

Um pr√©-processamento extremamente importante nesse caso √© a padroniza√ß√£o dos valores de entrada. Como as caracter√≠sticas variam em intervalos diferentes (cor: [1, 13], √°lcool: [11, 15]), elas v√£o exercer diferentes influ√™ncias sobre o nosso modelo e prejudicar muito a sua converg√™ncia. 

* **Ao final dessa aula, sugiro que rode esse script comentando a c√©lula abaixo, e veja o impacto sobre o processo de otimiza√ß√£o.**
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data = scaler.fit_transform(data)

plt.scatter(data[:, 0], data[:,1], c=targets, s=15, cmap=plt.cm.brg)
plt.xlabel(wine.feature_names[features[0]])
plt.ylabel(wine.feature_names[features[1]])

"""## Instanciando sua rede

Antes de entrar nas nuances da otimiza√ß√£o, vamos fazer o que j√° sabemos: instanciar um MLP de duas camadas neurais, uma escondida e outra de sa√≠da.

Lembre-se de definir o dispositivo de hardware (cuda ou cpu) antes de iniciar os trabalhos. 

"""

import torch
from torch import nn

torch.manual_seed(42)

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')

print(device)

input_size  = data.shape[1]
hidden_size = 32
out_size    = len(wine.target_names) # numero de classes 

net = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, out_size),
    nn.Softmax()
)

net = net.to(device)

"""## Visualizando a fronteira de decis√£o

Para facilitar o entendimento do processo de otimiza√ß√£o, vamos utilizar uma fun√ß√£o auxiliar para visualizar a fronteira de decis√£o da nossa rede neural de classifica√ß√£o. Como acabamos de instanci√°-la, seus pesos modelam uma fun√ß√£o aleat√≥ria que n√£o se ajusta adequadamente aos dados, e isso √© facilmente vis√≠vel em duas dimens√µes.

Adaptada de:<br>
https://github.com/camilalaranjeira/Neural-Lectures/blob/master/XOR_Problem.ipynb
"""

import numpy as np 

def plot_boundary(X, y, model):
  x_min, x_max = X[:, 0].min()-0.1, X[:, 0].max()+0.1
  y_min, y_max = X[:, 1].min()-0.1, X[:, 1].max()+0.1
  
  spacing = min(x_max - x_min, y_max - y_min) / 100
  
  XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),
                       np.arange(y_min, y_max, spacing))
  
  data = np.hstack((XX.ravel().reshape(-1,1), 
                    YY.ravel().reshape(-1,1)))
  
  # For binary problems
  # db_prob = model(Variable(torch.Tensor(data)).cuda() )
  # clf = np.where(db_prob.cpu().data < 0.5,0,1)
  
  # For multi-class problems
  db_prob = model(torch.Tensor(data).to(device) )
  clf = np.argmax(db_prob.cpu().data.numpy(), axis=-1)
  
  Z = clf.reshape(XX.shape)
  
  plt.contourf(XX, YY, Z, cmap=plt.cm.brg, alpha=0.5)
  plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', s=25, cmap=plt.cm.brg)

plot_boundary(data, targets, net)

"""## Pacote ```torch.optim```

M√£os a obra! Vamos agora otimizar a nossa rede usando os algoritmos mais tradicionais da √°rea. Para isso, a biblioteca ```torch.optim``` nos ser√° bem √∫til, pois ela implementa os principais algoritmos de otimiza√ß√£o de redes neurais.

O primeiro passo √© instanciar o otimizador. De acordo com o pacote ```optim```, basta chamar o otimizador escolhido, passando como par√¢metro:
* Os par√¢metros da rede que ser√° otimizada (```net.parameters()```)
* A taxa de aprendizado

A depender do otimizador, pode ser necess√°rio alimentar outros par√¢metros, mas esses dois s√£o obrigat√≥rios!

Vamos utilizar a **Descida do Gradiente** que vimos na aula te√≥rica, implementada pelo otimizador **```optim.SGD```** (*Stochastic Gradient Descent*).
"""

from torch import optim

# Fun√ß√£o de Perda
criterion = nn.CrossEntropyLoss().to(device)

# Otimizador: Descida do Gradiente
# Stochastic Gradient Descent
optimizer = optim.SGD(net.parameters(), lr=1e-3)

"""### Hiperpar√¢metros

* Valores definidos antes do in√≠cio do aprendizado
* Devem ser ajustados para cada tarefa espec√≠fica

A taxa de aprendizado n√£o √© o primeiro hiperpar√¢metro que temos contato. Ao definir a arquitetura da sua rede (quantos neur√¥nios e quantas camadas) voc√™ tamb√©m teve que escolher um valor adequado. **Essa escolha pode ser emp√≠rica, mas em geral deve ser experimental**, na busca pelo melhor conjunto de hiperpar√¢metros para solucionar o seu problema! 

* üêº Emp√≠rica ([Abordagem do Panda](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)): O programador investe seus esfor√ßos em um √∫nico modelo (um beb√™ panda), e altera os hiperpar√¢metros com base na sua experi√™ncia e nas observa√ß√µes
* üíª Experimental: M√∫ltiplos modelos s√£o gerados simultaneamente, com diferentes combina√ß√µes de hiperpar√¢metros. Dentre eles, √© escolhido o que apresentar melhor performance.

## Cast do dados

Os dados carregados do Scikit-Learn s√£o retornados como ```ndarrays```, por isso precisamos convert√™-los para tensores e carreg√°-los na GPU (caso dispon√≠vel) antes de alimentar o modelo neural.
"""

X = torch.FloatTensor(data).to(device) # GPU
Y = torch.LongTensor(targets).to(device)

"""## Treinando um modelo

O treinamento consiste nas etapas que vimos na aula te√≥rica. Aqui vamos relacionar cada etapa ao seu c√≥digo correspondente.

* Forward
  * Alimentar os dados para a rede <br>
  ```pred = net(X)```
  * Calcular a fun√ß√£o de custo <br>
  ```loss = criterion(pred, y)```
* Backpropagation
  * Calcular o gradiente <br>
  ```loss.backward()```
  * Atualizar os pesos <br>
  ```optimizer.step()```


A princ√≠pio n√£o vamos falar de procedimentos adequados de treinamento. Vamos apenas realizar as etapas de treinamento e ver o que acontece.
"""

for i in range(200):
  # Forward 
  pred = net(X)
  loss = criterion(pred, Y)

  # Backward
  loss.backward()
  optimizer.step()

  if i % 10 == 0:
    plt.figure()
    plot_boundary(data, targets, net)

"""## Treinamento: Conclus√£o

Como a otimiza√ß√£o de uma rede neural √© um processo iterativo, os dados de treino devem ser alimentados m√∫ltiplas vezes para o modelo. **Cada itera√ß√£o onde o conjunto de treino inteiro foi utilizado no processo de treinamento √© chamado de <font color='color'>√©poca</font>.** Veremos essas nomenclaturas em detalhes nas aulas futuras.
"""



"""# Carregamento de Dados

Objetivos dessa aula:
* Carregar dados reais do Pytorch
* Implementar o fluxo de treinamento completo de uma rede

Mas calma que essa ainda n√£o √© a linha de chegada. Ainda precisaremos falar do fluxo de valida√ß√£o.

## Hiperpar√¢metros

Agora que a brincadeira est√° ficando s√©ria, que tal uma sugest√£o de como organizar o seu c√≥digo? Para facilitar o entendimento e manuten√ß√£o do c√≥digo, mantenha sempre no in√≠cio os seguintes elementos:
* imports de pacotes
* configura√ß√£o de hiperpar√¢metros
* defini√ß√£o do hardware padr√£o utilizado

Nessa aula vamos trabalhar com dados reais, ent√£o **vamos precisar de GPU!** Ent√£o n√£o se esque√ßa de mudar as configura√ß√µes desse ambiente do colab. <br>
Sugiro rodar esse mesmo c√≥digo sem GPU em outro momento, s√≥ pra sentir o gostinho de como a GPU facilitou o uso de redes neurais.
"""

import torch
from torch import nn, optim

from torchvision import datasets
from torchvision import transforms 

from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import numpy as np
import time

args = {
    'batch_size': 5,
    'num_workers': 4,
    'num_classes': 10,
    'lr': 1e-4,
    'weight_decay': 5e-4,
    'num_epochs': 30
}

if torch.cuda.is_available():
  args['device'] = torch.device('cuda')
else:
  args['device'] = torch.device('cpu')

print(args['device'])

"""## Datasets

O PyTorch possui dois pacotes que trazem datasets prontos para uso.

* Torchtext: https://torchtext.readthedocs.io/en/latest/datasets.html
* Torchvision: https://pytorch.org/docs/stable/torchvision/datasets.html

Como os nomes indicam, s√£o datasets de textos (text) e imagens (vision), duas aplica√ß√µes onde redes neurais s√£o muito bem sucedidas.

Para aplica√ß√µes com textos e outros tipos de s√©ries temporais, o carregamento de dados possui nuances que dificultam o entendimento, portanto vamos concentrar no carregamento de imagens.

### Torchvision datasets

Para trabalhar com datasets do pacote torchvision, basta
* Importar o pacote
``` python 
from torchvision import datasets 
```
* Carregar o dataset do seu interesse (ex: MNIST)
``` python 
data = datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)
```

Documenta√ß√£o: https://pytorch.org/docs/stable/torchvision/datasets.html

### Torchvision transforms

N√£o vamos entrar em detalhes sobre transforma√ß√µes de imagens, mas para qualquer dataset √© necess√°rio transform√°-lo em tensor para que possamos alimentar uma rede em pytorch. Isso pode ser feito no carregamento dos dados, basta:

* Importar o pacote transforms
``` python 
from torchvision import transforms 
```
* preencher o par√¢metro ```tranform``` do dataset com a fun√ß√£o que converte para tensor.
``` python 
transforms.ToTensor() 
```

Pronto! Quando seu dado for carregado, ele passar√° pela transforma√ß√£o indicada no par√¢metro ```tranform```, nesse caso, convertendo o dado para um tensor.

Documenta√ß√£o: https://pytorch.org/docs/stable/torchvision/transforms.html

"""

train_set = datasets.MNIST('./', 
                           train=True, 
                           transform=transforms.ToTensor(),
                           download=True)

test_set = datasets.MNIST('./', 
                           train=False, 
                           transform=transforms.ToTensor(),
                           download=False)

print('Amostras de treino: ' + str(len(train_set)) + '\nAmostras de Teste:' + str(len(test_set)))

"""Cada dataset possui uma implementa√ß√£o espec√≠fica internamente no pytorch. Verifique o ```type``` da vari√°vel que recebeu os dados e veja que se refere a uma classe espec√≠fica do dataset.

No entanto, o item de qualquer dataset **sempre ser√° uma tupla ```(dado, r√≥tulo)```**. 
"""

print(type(train_set))
print(type(train_set[0]))

"""Podemos ent√£o iterar no dataset para observar algumas amostras e seus r√≥tulos."""

for i in range(3):
  dado, rotulo = train_set[i]

  plt.figure()
  plt.imshow(dado[0])
  plt.title('Rotulo: '+ str(rotulo))

"""Temos um total de 70 mil amostras, mas elas **ainda n√£o est√£o carregadas na mem√≥ria** (isso seria bastante custoso). A vantagem da classe ```Dataset``` do Pytorch √© que as amostras s√≥ s√£o carregadas quando necess√°rio.

Sugest√£o: experimente trocar a transforma√ß√£o do Dataset para
```python
transforms.RandomCrop(12)
```
Essa fun√ß√£o realiza um recorte aleat√≥rio de ```12 x 12``` (pixels) na imagem. Ao carregar a mesma amostra m√∫ltiplas vezes, um novo recorte ser√° feito. 
"""

crop_set = datasets.MNIST('./', 
                           train=False, 
                           transform=transforms.RandomCrop(12),
                           download=False)

# Tuple (dado, r√≥tulo)
for i in range(3):
  dado, rotulo = crop_set[0]
  
  plt.figure()
  plt.imshow(dado)
  plt.title('R√≥tulo: '+ str(rotulo))

"""Em resumo, cada vez que indexamos um item do dataset, as seguintes opera√ß√µes s√£o realizadas:
* Amostra lida do arquivo e carregada como uma tupla ```(dado, r√≥tulo)```
* As transforma√ß√µes s√£o aplicadas

## Dataloader

Essa aqui √© uma das principais raz√µes do Pytorch ser o pacote preferido de muitos profissionais. O Dataloader gerencia muito bem o carregamento de dados para o treinamento de redes neurais, trazendo as fun√ß√µes: 

* Separa√ß√£o dos dados em batches
* Embaralhando os dados
* Carregando batches em paralelo utilizando threads

O uso de threads no carregamento minimiza per√≠odos ociosos de processamento, visto que a leitura de dados em arquivo √© um grande gargalo de tempo.

As tr√™s funcionalidades que acabamos de conhecer s√£o controladas pelos par√¢metros da chamada do DataLoader.
```python
loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)
```
"""

train_loader = DataLoader(train_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True, 
                          num_workers=args['num_workers'])

test_loader = DataLoader(test_set, 
                          batch_size=args['batch_size'], 
                          shuffle=True, 
                          num_workers=args['num_workers'])

"""O objeto retornado √© um **iterador**, podendo ser utilizado para iterar em loops mas n√£o suportando indexa√ß√£o."""

for batch in train_loader:
  
  dado, rotulo = batch
  print(dado.size(), rotulo.size())

  plt.imshow(dado[0][0])
  plt.title('Rotulo: '+ str(rotulo[0]) )
  break

"""## Implementando o MLP


**Lembrete**: Multi-Layer Perceptrons trabalham somente com dados unidimensionais (vetores). Sendo a imagem com dimensionalidade ```(1, 28, 28)```, precisamos lineariz√°-la antes de alimentar a rede. Isso implica que o a entrada da rede ter√° ```input_size = 28 x 28 x 1 = 784```
"""

class MLP(nn.Module):

  def __init__(self, input_size, hidden_size, out_size):
    super(MLP, self).__init__()

    self.features  = nn.Sequential(
                      nn.Linear(input_size, hidden_size),
                      nn.ReLU(),
                      nn.Linear(hidden_size, hidden_size),
                      nn.ReLU()
                    )
    self.out     = nn.Linear(hidden_size, out_size)
    self.softmax = nn.Softmax()

  def forward(self, X):
    
    X = X.view(X.size(0), -1)

    feature = self.features(X)
    output  = self.softmax(self.out(feature))

    return output

input_size  = 28 * 28
hidden_size = 128
out_size    = 10 #classes

torch.manual_seed(42)
net = MLP(input_size, hidden_size, out_size).to(args['device']) #cast na GPU

"""## Definindo loss e otimizador"""

criterion = nn.CrossEntropyLoss().to(args['device'])
optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])



"""# Fluxo de Treinamento

Agora vamos aplicar o conhecimento que acabamos de aprender!

Relembrando o passo a passo do fluxo de treinamento:
* Iterar nas √©pocas
* Iterar nos batches
* Cast dos dados no dispositivo de hardware
* Forward na rede e c√°lculo da loss
* C√°lculo do gradiente e atualiza√ß√£o dos pesos

Para acompanhar a converg√™ncia do seu modelo (e garantir que tudo foi feito certinho), ao final de cada √©poca podemos imprimir a m√©dia e o desvio padr√£o das perdas de cada itera√ß√£o.
"""

for epoch in range(args['num_epochs']):
  start = time.time()

  epoch_loss = []
  for batch in train_loader:
    
    dado, rotulo = batch

    # Cast na GPU
    dado   = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])

    # Forward 
    pred = net(dado)
    loss = criterion(pred, rotulo)
    epoch_loss.append(loss.cpu().data)

    # Backward
    loss.backward()
    optimizer.step()

  epoch_loss = np.asarray(epoch_loss)
  end = time.time()

  print("Epoca %d, Loss: %.4f +\- %.4f, Tempo: %.2f" % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start) )

"""### Interpretando a qualidade do modelo

Apesar da loss ser o crit√©rio utilizado na otimiza√ß√£o do modelo, seu valor √© pouco interpret√°vel por seres humanos.

* *A loss est√° melhorando, mas como saber se meu modelo est√° bom mesmo?*

Podemos calcular uma m√©trica mais interpret√°vel. No caso da classifica√ß√£o, temos a acur√°cia como m√©trica mais simples de avalia√ß√£o. Esse valor ser√° usado exclusivamente para visualizar a qualidade do modelo, n√£o interferindo no treinamento da rede. 

Na c√©lula anterior utilizaremos a biblioteca Scikit-Learn para calcular a acur√°cia, para isso basta
* Armazenar os r√≥tulos de cada itera√ß√£o
* Calcular a predi√ß√£o a partir da sa√≠da da rede
* Utilizar a fun√ß√£o do sklearn para c√°lculo da acur√°cia
```python
metrics.accuracy_score(rotulos, ypreds)
```

Documenta√ß√£o: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
"""



"""# Carregamento de Dados

Objetivos dessa aula:
* Carregar um dataset customizado
* Implementar o fluxo de treinamento **e valida√ß√£o** completo de uma rede

## Hiperpar√¢metros

Vamos manter a organiza√ß√£o do √∫ltimo script :)

* imports de pacotes
* configura√ß√£o de hiperpar√¢metros
* defini√ß√£o do hardware padr√£o utilizado

E bora de GPU de novo!
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
from torch import optim

from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from sklearn import metrics
from sklearn.preprocessing import StandardScaler

import pandas as pd
import numpy as np
import time
import os


import matplotlib.pyplot as plt
# %matplotlib inline

# Configurando hiperpar√¢metros.
args = {
    'epoch_num': 200,     # N√∫mero de √©pocas.
    'lr': 5e-5,           # Taxa de aprendizado.
    'weight_decay': 5e-4, # Penalidade L2 (Regulariza√ß√£o).
    'num_workers': 3,     # N√∫mero de threads do dataloader.
    'batch_size': 20,     # Tamanho do batch.
}

if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')

print(args['device'])

"""## Dataset 

Dataset de aplicativos para aluguel de bicicletas (*Bike Sharing Dataset*). <br>
* Dadas algumas informa√ß√µes como velocidade do vento, esta√ß√£o do ano, etc., quantas bicicletas ser√£o alugadas na pr√≥xima hora?

Esse √© um problema de **Regress√£o**, onde precisamos estimar uma vari√°vel dependente em um espa√ßo cont√≠nuo (alugueis de bikes) a partir de um conjunto de vari√°veis independentes (as condi√ß√µes no momento).

### Baixando o dataset

Fonte: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
"""

! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip
! unzip Bike-Sharing-Dataset.zip

"""### Visualizando os dados"""

df = pd.read_csv('hour.csv')
print(len(df))
df.head()

"""### Tratamento de dados

**Vari√°veis Categ√≥ricas** <br>
Como descrito na p√°gina do dataset, apenas as vari√°veis num√©ricas est√£o normalizadas. No caso das categ√≥ricas (como dia da semana e esta√ß√£o do ano), cada elemento cont√©m o √≠ndice da categoria.

Existem v√°rias formas de lidar com vari√°veis categ√≥ricas em uma regress√£o, mas para n√£o desviar o foco da nossa aula manteremos os valores originais das vari√°veis categ√≥ricas.

**Separa√ß√£o em treino e teste**<br>

Para treinar e validar o nosso modelo, precisamos de dois conjuntos de dados (treino e teste). Para isso, utilizaremos a fun√ß√£o ```torch.randperm``` para amostrar aleatoriamente um percentual dos dados, separando-os para valida√ß√£o.

Documenta√ß√£o: https://pytorch.org/docs/stable/torch.html#torch.randperm
"""

# Train/Test split
torch.manual_seed(1)
indices = torch.randperm(len(df)).tolist()

train_size = int(0.8*len(df))
df_train = df.iloc[indices[:train_size]]
df_test  = df.iloc[indices[train_size:]]

print(len(df_train), len(df_test))
display(df_test.head())

df_train.to_csv('bike_train.csv',index=False)
df_test.to_csv('bike_test.csv',index=False)
!ls

"""### Classe Dataset

O pacote ```torch.util.data``` possui a classe abstrata ```Dataset```. Ela permite que voc√™ implemente o seu pr√≥prio dataset reescrevendo os m√©todos:

* ```__init__(self)```: Define a lista de amostras do seu dataset
* ```__getitem__(self, idx)```: Carrega uma amostra, aplica as devidas transforma√ß√µes e retorna uma **tupla ```(dado, r√≥tulo)```**.
* ```__len__(self)```: Retorna a quantidade de amostras do dataset

Tutorial completo do PyTorch: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html

"""

class Bicicletinha(Dataset):
  def __init__(self, csv_path, scaler_feat=None, scaler_label=None):
  
    self.dados = pd.read_csv(csv_path).to_numpy()
    
  def __getitem__(self, idx):
    
    sample = self.dados[idx][2:14]
    label  = self.dados[idx][-1:]
    
    # converte para tensor
    sample = torch.from_numpy(sample.astype(np.float32))
    label  = torch.from_numpy(label.astype(np.float32))
    
    return sample, label
    
  def __len__(self):
    return len(self.dados)

dataset = Bicicletinha('bike_train.csv')
dado, rotulo = dataset[0]
print(rotulo)
print(dado)

"""### Construindo conjuntos de treino e teste"""

train_set = Bicicletinha('bike_train.csv')
test_set  = Bicicletinha('bike_test.csv')

print('Tamanho do treino: ' + str(len(train_set)) + ' amostras')
print('Tamanho do teste: ' + str(len(test_set)) + ' amostras')

"""## Dataloader

"""

# Criando dataloader
train_loader = DataLoader(train_set,
                          args['batch_size'],
                          num_workers=args['num_workers'],
                          shuffle=True)
test_loader = DataLoader(test_set,
                         args['batch_size'],
                         num_workers=args['num_workers'],
                         shuffle=False)

"""O objeto retornado √© um **iterador**, podendo ser utilizado para iterar em loops mas n√£o suportando indexa√ß√£o."""

for batch in test_loader:
  
  dado, rotulo = batch
  print('## Dimensionalidade do batch ##')
  print(dado.size(), rotulo.size())
  
  break

"""## Implementando o MLP

Essa parte aqui voc√™ j√° tira de letra! Minha sugest√£o √© construir um modelo com:

* **Duas camadas escondidas**. Lembre-se de alternar as camadas com ativa√ß√µes n√£o-lineares. 
* Uma camada de sa√≠da (com qual ativa√ß√£o?)
"""

class MLP(nn.Module):
  
  def __init__(self, input_size, hidden_size, out_size):
    super(MLP, self).__init__()
    
    self.features = nn.Sequential(
          nn.Linear(input_size, hidden_size),
          nn.ReLU(),
          nn.Linear(hidden_size, hidden_size),
          nn.ReLU(),
    )
    
    self.classifier = nn.Sequential(
        nn.Linear(hidden_size, out_size),
        nn.ReLU(),
    )

  def forward(self, X):
    
    hidden = self.features(X)
    output = self.classifier(hidden)
    
    return output

input_size  = train_set[0][0].size(0)
hidden_size = 128
out_size    = 1

net = MLP(input_size, hidden_size, out_size).to(args['device'])
print(net)

"""## Definindo loss e otimizador

Se lembra quais as fun√ß√µes de perda adequadas para um problema de regress√£o?
"""

criterion = nn.L1Loss().to(args['device'])

optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])



"""# Fluxo de Treinamento & Valida√ß√£o

## Treinamento

Relembrando o passo a passo do fluxo de treinamento:
* Iterar nas √©pocas
* Iterar nos batches
* Cast dos dados no dispositivo de hardware
* Forward na rede e c√°lculo da loss
* C√°lculo do gradiente e atualiza√ß√£o dos pesos

Esse conjunto de passos √© respons√°vel pelo processo iterativo de otimiza√ß√£o de uma rede. **A valida√ß√£o** por outro lado, √© apenas a aplica√ß√£o da rede em dados nunca antes visto para estimar a qualidade do modelo no mundo real.

## Valida√ß√£o

Para essa etapa, o PyTorch oferece dois artif√≠cios:
* ```model.eval()```: Impacta no *forward* da rede, informando as camadas caso seu comportamento mude entre fluxos (ex: dropout).
* ```with torch.no_grad()```: Gerenciador de contexto que desabilita o c√°lculo e armazenamento de gradientes (economia de tempo e mem√≥ria). Todo o c√≥digo de valida√ß√£o deve ser executado dentro desse contexto.

Exemplo de c√≥digo para valida√ß√£o

```python
net.eval()
with torch.no_grad():
  for batch in test_loader:
      # C√≥digo de valida√ß√£o
```

Existe o equivalente ao ```model.eval()``` para explicitar que a sua rede deve estar em modo de treino, √© o ```model.train()```. Apesar de ser o padr√£o dos modelos, √© boa pr√°tica definir tamb√©m o modo de treinamento.
"""

def train(train_loader, net, epoch):

  # Training mode
  net.train()
  
  start = time.time()
  
  epoch_loss  = []
  for batch in train_loader:
    
    dado, rotulo = batch
    
    # Cast do dado na GPU
    dado = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])
    
    # Forward
    ypred = net(dado)
    loss = criterion(ypred, rotulo)
    epoch_loss.append(loss.cpu().data)
    
    # Backpropagation
    loss.backward()
    optimizer.step()
   
  epoch_loss = np.asarray(epoch_loss)
  
  end = time.time()
  print('#################### Train ####################')
  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))
  
  return epoch_loss.mean()

def validate(test_loader, net, epoch):

  # Evaluation mode
  net.eval()
  
  start = time.time()
  
  epoch_loss  = []
  
  with torch.no_grad(): 
    for batch in test_loader:

      dado, rotulo = batch

      # Cast do dado na GPU
      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Forward
      ypred = net(dado)
      loss = criterion(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

  epoch_loss = np.asarray(epoch_loss)
  
  end = time.time()
  print('********** Validate **********')
  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))
  
  return epoch_loss.mean()

train_losses, test_losses = [], []
for epoch in range(args['epoch_num']):
  
  # Train
  train_losses.append(train(train_loader, net, epoch))
  
  # Validate
  test_losses.append(validate(test_loader, net, epoch))

Xtest = torch.stack([tup[0] for tup in test_set])
Xtest = Xtest.to(args['device'])

ytest = torch.stack([tup[1] for tup in test_set])
ypred = net(Xtest).cpu().data

data = torch.cat((ytest, ypred), axis=1)

df_results = pd.DataFrame(data, columns=['ypred', 'ytest'])
df_results.head(20)

"""# Gr√°fico de converg√™ncia"""

plt.figure(figsize=(20, 9))
plt.plot(train_losses, label='Train')
plt.plot(test_losses, label='Test', linewidth=3, alpha=0.5)
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Loss', fontsize=16)
plt.title('Convergence', fontsize=16)
plt.legend()
plt.show()